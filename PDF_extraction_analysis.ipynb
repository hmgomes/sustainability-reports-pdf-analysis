{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading PDF files and basic NLP\n",
    "# import PyPDF2\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import io\n",
    "\n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Fuzzy string match\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# To generate word clouds\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Read unicode data from the extra_stop_words file\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Frequency counting and collections\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# To navigate through all files in a directory\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define functions for the text extraction, preprocessing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://gist.github.com/boniattirodrigo/67429ada53b7337d2e79\n",
    "def remove_special_characters(word, only_numbers_letters_spaces=False):\n",
    "\n",
    "    # Unicode normalize transforma um caracter em seu equivalente em latin.\n",
    "    nfkd = unicodedata.normalize('NFKD', word)\n",
    "    plain_word = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "\n",
    "    # return the word with only numbers, letters and spaces\n",
    "    if only_numbers_letters_spaces:\n",
    "        return re.sub('[^a-zA-Z0-9 \\\\\\]', '', plain_word)\n",
    "    return plain_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f\n",
    "# Updated the keywords removal \n",
    "def extract_keywords(path_pdf,\n",
    "                     pdf_file,\n",
    "                     path_output_raw_text,\n",
    "                     language,#='portuguese', \n",
    "                     path_extra_stop_words,\n",
    "                     file_extra_stop_words\n",
    "                    ): \n",
    "    # The method used to transform the pdf to text ('pdfminer.six' or 'textract_OCR')\n",
    "    method = ''\n",
    "    # Check if the raw text is not already availabe in the path_output_raw_text\n",
    "    text = ''\n",
    "    try:\n",
    "        candidate_raw = open(os.path.join(path_output_raw_text, pdf_file.replace('.pdf', '.txt')), 'r')\n",
    "        text = candidate_raw.read()\n",
    "#         print('successfully read the raw text! ')\n",
    "    except IOError:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle)\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "        with open(os.path.join(path_pdf, pdf_file), 'rb') as candidate_raw:\n",
    "            for page in PDFPage.get_pages(candidate_raw, \n",
    "                                          caching=True,\n",
    "                                          check_extractable=True):\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "            text = fake_file_handle.getvalue()\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "        \n",
    "        if text:\n",
    "            method = 'pdfminer.six'\n",
    "            text = text\n",
    "        # If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "        else: # fileurl\n",
    "            method = 'textract_OCR'\n",
    "#             print('Activating OCR library for {0} file (language = {1})'.format(pdf_file,language))\n",
    "            if language == 'portuguese':\n",
    "                text = textract.process(os.path.join(path_pdf, pdf_file), method='tesseract', language='por', encoding='utf8')\n",
    "            else:\n",
    "                text = textract.process(os.path.join(path_pdf, pdf_file), method='tesseract', language='eng', encoding='utf8')\n",
    "\n",
    "        file_raw_text = open(os.path.join(path_output_raw_text, pdf_file.replace('.pdf', '.txt')), 'w', encoding='utf8')\n",
    "        file_raw_text.write(text)\n",
    "    \n",
    "    text = remove_special_characters(text)\n",
    "    # The word_tokenize() function will break our text phrases into #individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '%', '-', '.', '|', '']\n",
    "    stop_words = stopwords.words(language)\n",
    "    extra_stop_words = open(os.path.join(path_extra_stop_words, file_extra_stop_words), 'r', encoding='utf8').read().split('\\n')\n",
    "    \n",
    "    # We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN\n",
    "    # punctuations.\n",
    "    keywords = [word.lower() for word in tokens \n",
    "                if \n",
    "                not word.lower() in stop_words and \n",
    "                not word.lower() in punctuations and \n",
    "                not word.lower() in extra_stop_words and \n",
    "                word.isalpha() and\n",
    "                len(word) >= 2]\n",
    "    \n",
    "    return np.asarray(keywords), method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cloud_from_keywords_frequency(keywords_frequency, file_name, path='./wordclouds/', show_image=False):\n",
    "    wordcloud = WordCloud(width = 512, height = 512, background_color='white')\n",
    "    fig = plt.figure(figsize=(20,16),facecolor = 'white', edgecolor='blue')\n",
    "    plt.imshow(wordcloud.generate_from_frequencies(keywords_frequency), interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.savefig(path+file_name)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_statistics(keywords):\n",
    "    total_words = 0\n",
    "    keywords_dict = dict(collections.Counter(keywords).most_common())\n",
    "    for k in keywords_dict:\n",
    "        total_words += keywords_dict[k]\n",
    "\n",
    "    stats = dict()\n",
    "    for k in keywords_dict:\n",
    "        stats[k] = {'count': keywords_dict[k], 'text_frequency': keywords_dict[k]/float(total_words)}    \n",
    "#     stats['frequency'].most_common(10)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_report(path, output_file, keyword_stats):\n",
    "    output = open(path+output_file, 'w', encoding='utf8')\n",
    "    header = u'word,count,text_frequency\\n'\n",
    "    output.write(header)\n",
    "    for k in keyword_stats:\n",
    "        line = u'%s,%d,%.5f\\n' % (k, keyword_stats[k]['count'], keyword_stats[k]['text_frequency'])\n",
    "        output.write(line)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count occurrences of any word\n",
    "\n",
    "* All PDFs in a given path are processed.\n",
    "* This function generates 'raw reports', counting the occurrence and frequency of every word of the document. \n",
    "* A word is defined as a sequence of characters delimited by empty spaces, i.e. ' '. \n",
    "* **TODO:** This version of the code still doesn't handle pdfs that are in PT and EN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_PDFs(path='./data/', # dir containing the subdirs per language\n",
    "                     raw_text_path='./raw_texts/EN/',\n",
    "                     output_path='./reports/EN/', \n",
    "                     wordclouds_path='./wordclouds/EN/',\n",
    "                     language='english',\n",
    "                     path_extra_stop_words='./',\n",
    "                     file_extra_stop_words='extra_stop_words_EN.txt',\n",
    "                     wordclouds=False, \n",
    "                     start_index=0):\n",
    "    \n",
    "    num_pdfminersix = 0\n",
    "    num_textract_OCR = 0\n",
    "    # We assume there are only pdfs in this directory\n",
    "    PDFs = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    PDFs = list(filter(lambda pdf: pdf.find('.pdf') != -1, PDFs))\n",
    "        \n",
    "    counter = start_index\n",
    "    for pdf in PDFs[counter:]:\n",
    "        \n",
    "        print('Processing PDFs ({0}) {1} of {2} ({3})'.format(language, counter+1, len(PDFs), pdf))\n",
    "        keywords, method = extract_keywords(path, pdf, \n",
    "                                    path_output_raw_text=raw_text_path, \n",
    "                                    language=language,\n",
    "                                    path_extra_stop_words=path_extra_stop_words,\n",
    "                                    file_extra_stop_words=file_extra_stop_words)\n",
    "        if method == 'pdfminer.six':\n",
    "            num_pdfminersix += 1\n",
    "        else:\n",
    "            num_textract_OCR += 1\n",
    "        keywords_statistics = calculate_statistics(keywords)\n",
    "        write_report(output_path, pdf.replace('.pdf', '.csv'), keywords_statistics)\n",
    "        counter = counter + 1\n",
    "        if wordclouds:\n",
    "            word_cloud_from_keywords_frequency(collections.Counter(keywords), \n",
    "                                               pdf.replace('.pdf', '.png'), \n",
    "                                               path=wordclouds_path,\n",
    "                                               show_image=False)\n",
    "    print('Stats for {0}: num_pdfminersix = {1}, num_textract_OCR = {2}'\n",
    "          .format(raw_text_path, num_pdfminersix, num_textract_OCR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Executing the PDF processing\n",
    "This code may take a while to finish depending on the amount of PDF files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # ENglish PDFs processing\n",
    "# process_PDFs(path='/Users/hmg/Desktop/Data/pdfs_EN/', \n",
    "#              raw_text_path='./raw_texts/EN/',\n",
    "#              output_path='./reports/EN/', \n",
    "#              wordclouds_path='./wordclouds/EN/',\n",
    "#              language='english',\n",
    "#              wordclouds=True,\n",
    "#             start_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # PorTuguese PDFs processing\n",
    "# for i in range(1,5): # Hardcoded 5 because I know there are only 4 directories... \n",
    "#     print('Processing pdfs_PT{0}'.format(i))\n",
    "#     process_PDFs(path='/Users/hmg/Desktop/Data/pdfs_PT{0}/'.format(i), \n",
    "#              raw_text_path='./raw_texts/PT/',\n",
    "#              output_path='./reports/PT/', \n",
    "#              wordclouds_path='./wordclouds/PT/',\n",
    "#              language='portuguese',\n",
    "#              file_extra_stop_words='extra_stop_words_PT.txt',\n",
    "#              wordclouds=True,\n",
    "#              start_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Count occurrences of predefined phrases/words\n",
    "\n",
    "Count occurrences of predefined text using 'fuzzy string match', added ```word_match_count``` lambda object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def count_total_words(text, delim=' '): \n",
    "#     return len(remove_special_characters(text).split(delim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_normal(text, word, confidence = 0, phrases = False, debug = False, output = ''):\n",
    "    if phrases:\n",
    "        return text.count(word)\n",
    "    else:\n",
    "        counter = 0\n",
    "        for p in text.split(' '):\n",
    "            if(p == word):\n",
    "                counter = counter + 1\n",
    "        return counter\n",
    "\n",
    "def count_fuzzy(text, word, confidence = 95, phrases=False, debug = False, output = ''):\n",
    "    counter = 0\n",
    "    if len(word) == 0:\n",
    "        return 0\n",
    "    # Phrases\n",
    "    if phrases:\n",
    "        # Sliding window strategy: create a text_word from text with length word.len \n",
    "        #  and by moving 1 character at a time\n",
    "        #     If match, then skip the next word.len, just to avoid double counting!\n",
    "        i = 0\n",
    "        ## DEBUG\n",
    "#         print('count_fuzzy - word = {0} and, len(text) = {1}, confidence = {2}'.format(word, len(text), confidence))\n",
    "        if debug:\n",
    "            output.write('{0} ({1})'.format(word, confidence) + ',')\n",
    "#             print('{0} ({1})'.format(word, confidence))\n",
    "    \n",
    "        while i < len(text):\n",
    "            text_word_last_idx = i+len(word)\n",
    "            match_confidence = 0\n",
    "            if text_word_last_idx <= len(text):\n",
    "                match_confidence = fuzz.ratio(text[i:text_word_last_idx], word)\n",
    "#                 print('token_set_ratio({0},{1})={2}'.format(text[i:text_word_last_idx], word, match_confidence))\n",
    "                if match_confidence > confidence:\n",
    "                    counter += 1\n",
    "                    if debug:\n",
    "                        output.write('{0} ({1})'.format(text[i:text_word_last_idx], match_confidence) + ',')\n",
    "#                         print('{0} =~ {1} ({2})'.format(text[i:text_word_last_idx], word, match_confidence))\n",
    "                    i += len(word)\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                break\n",
    "            ## DEBUG - only about 100 characters\n",
    "#             if debug and (i % int(len(text)/10) == 0):\n",
    "#                 print('{} of {} = {:.2f}%'.format(i, len(text), i/len(text)*100))\n",
    "    else:\n",
    "        text_vec = text.split()\n",
    "        if debug:\n",
    "            output.write('{0} ({1})'.format(word, confidence) + ',')\n",
    "        for w in text_vec:\n",
    "            match_confidence = fuzz.ratio(w, word)\n",
    "            if match_confidence > confidence:\n",
    "                if debug:\n",
    "                    output.write('{0} ({1})'.format(w, match_confidence) + ',')\n",
    "                counter += 1\n",
    "    if debug:\n",
    "        output.write('\\n')\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_special_keywords(filter_list_path='./', \n",
    "                            filter_list_file='lista_palavras_EN.txt', \n",
    "                            raw_text_path='./raw_texts/EN/',\n",
    "                            output_path='./reports_special/EN/',\n",
    "                            output_file='output_EN.csv',\n",
    "                            language='english',\n",
    "                            path_extra_stop_words='./',\n",
    "                            file_extra_stop_words='extra_stop_words_EN.txt',\n",
    "                            word_match_count=count_normal,\n",
    "                            output_encoding='utf8',\n",
    "                            confidence=95,\n",
    "                            phrases=False,\n",
    "                            output_debug_path='./debug_fuzzy/EN/',\n",
    "                            debug=False,\n",
    "                            start_index=0,\n",
    "                            last_index=-1,\n",
    "                            preprocessing=False):\n",
    "    print('Starting filter_special_keywords')\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '%', '-', '.', '|', '']\n",
    "    stop_words = stopwords.words(language)\n",
    "    extra_stop_words = open(os.path.join(path_extra_stop_words, file_extra_stop_words), 'r', encoding='utf8').read().split('\\n')\n",
    "    \n",
    "    if not phrases:\n",
    "        print('[not phrases] Stopwords will be removed prior to processing the texts.')\n",
    "        \n",
    "    \n",
    "    filter_words = ''\n",
    "    try:\n",
    "        filter_words = open(os.path.join(filter_list_path, filter_list_file), 'r', encoding='utf8').read().split('\\n')\n",
    "        print('successfully read the filter words at {0} named {1}'.format(filter_list_path, filter_list_file))\n",
    "    except IOError:\n",
    "        print('failed to read the filter words at {0} named {1}'.format(filter_list_path, filter_list_file))\n",
    "    \n",
    "    # assuming there are only the raw texts in the directory\n",
    "    text_files = [f for f in listdir(raw_text_path) if isfile(join(raw_text_path, f))]\n",
    "    text_files = list(filter(lambda text_file: text_file.find('.txt') != -1, text_files))\n",
    "    \n",
    "    stats_per_file = {}\n",
    "    total_words_per_file = {}\n",
    "    \n",
    "    processed = start_index\n",
    "    if last_index == -1:\n",
    "        last_index = len(text_files)\n",
    "    \n",
    "    for text_file in text_files[processed:last_index]:\n",
    "        print('Processing Text ({0}) {1} of {2} ({3})'.format(language, processed+1, \n",
    "                                                              last_index, text_file))\n",
    "        debug_output = open(output_debug_path+'DEBUG_{0}_'.format(confidence)+text_file.replace('.txt','.csv'), 'w', encoding='utf8') \n",
    "        try:\n",
    "            text = open(os.path.join(raw_text_path, text_file), 'r').read()\n",
    "        except IOError:\n",
    "            print('Failed to open file at {0} named {1}'.format(raw_text_path, text_file))\n",
    "                \n",
    "        # Initialize the stats dictionary that will hold counters for the search words {'word': #occurrences}\n",
    "        stats = {}\n",
    "        \n",
    "        # Transform the original text, transform from unicode to ASCII (DOES NOT REMOVE SPECIAL) and set it to lower.\n",
    "        search_text = remove_special_characters(text).lower()\n",
    "        \n",
    "#         if preprocessing: PREPROCESSING WILL CERTAINLY BE USED FOR COUNTING NUMBER OF WORDS!!\n",
    "        # This remove stop_words, extra_words and punctuations before counting. \n",
    "        # TODO: merge this and the previous stop_word filter in a function. \n",
    "        search_text_nostopwords = [word for word in search_text.split(' ')\n",
    "                               if \n",
    "                               not word in stop_words and\n",
    "                               not word in punctuations and \n",
    "                               not word in extra_stop_words and\n",
    "                               len(word) >= 1]\n",
    "        \n",
    "        if preprocessing:\n",
    "            # If NOT phrases, then it is safe to remove stop words from the search_text.\n",
    "            # That is an assumption! Maybe we run into a search word that is a stop word!\n",
    "            if not phrases:\n",
    "                search_text = ' '.join(search_text_nostopwords)\n",
    "                \n",
    "        # Update to ALWAYS count as total words after removing stop_words. \n",
    "        stats['@TOTAL_WORDS'] = len(search_text_nostopwords)\n",
    "#         else:\n",
    "#             stats['@TOTAL_WORDS'] = len(search_text.split())\n",
    "        \n",
    "        for filter_word in filter_words:\n",
    "            # Transform the filter_word, transform from unicode to ASCII (DOES NOT REMOVE SPECIAL) and set it to lower.\n",
    "            search_word = remove_special_characters(filter_word).lower()\n",
    "            stats[filter_word] = word_match_count(search_text, search_word, confidence, phrases, debug, debug_output)\n",
    "            # text.lower().count(filter_word.lower())\n",
    "#             line = u'%s,%d,%.5f\\n' % (filter_word, counter, -1)\n",
    "\n",
    "        stats_per_file[text_file.replace('.txt','')] = stats\n",
    "        \n",
    "        processed = processed + 1\n",
    "        \n",
    "    output = open(output_path+output_file, 'w', encoding=output_encoding)   \n",
    "    header = u',' + u','.join(text_files[start_index:last_index])+u'\\n'\n",
    "    header = header + u'words \\ total_words_per_file,' + u','.join(total_words_per_file)\n",
    "    output.write(header)\n",
    "    \n",
    "    for text_file in text_files[start_index:last_index]:\n",
    "        output.write(str(stats_per_file[text_file.replace('.txt','')]['@TOTAL_WORDS']) + ',')\n",
    "    output.write('\\n')\n",
    "    \n",
    "    for filter_word in filter_words:\n",
    "        output.write(filter_word + ',')\n",
    "        for text_file in text_files[start_index:last_index]:\n",
    "#             line = u'%s,%d,%.5f\\n' % (filter_word, counter, -1)\n",
    "            output.write(str(stats_per_file[text_file.replace('.txt','')][filter_word]) + ',')\n",
    "        output.write('\\n')\n",
    "    output.close()\n",
    "    print('Finishing filter_special_keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ + ~ + ~ + ~ + ~ + ~ + ~\n",
    "# Filter the special words\n",
    "# ~ + ~ + ~ + ~ + ~ + ~ + ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 EN 'exact match'\n",
    "\n",
    "\n",
    "### <span style=\"color:blue\">PREPROCESSING=OFF</span>\n",
    "### <span style=\"color:blue\">EXACT MATCH</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# filter_special_keywords(filter_list_path='./', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN/',\n",
    "#                             output_path='./reports_special/EN/',\n",
    "#                             output_file='relatorio_normal_frases_EN.csv',\n",
    "#                             word_match_count=count_normal,\n",
    "#                             confidence=-1,\n",
    "#                             phrases=True,\n",
    "#                             preprocessing=False\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 EN <span style=\"color:red\">'fuzzy match'</span>\n",
    "\n",
    "\n",
    "#### PREPROCESSING=OFF\n",
    "# <span style=\"color:red\">FUZZY MATCH (90%)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CONFIDENCE = 90\n",
    "START_INDEX = 0\n",
    "LAST_INDEX = -1\n",
    "\n",
    "# filter_special_keywords(filter_list_path='./', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN/',\n",
    "#                             output_path='./reports_special/EN/',\n",
    "#                             output_file='relatorio_aproximado({0})_frases_EN_{1}-{2}.csv'\n",
    "#                                         .format(CONFIDENCE, START_INDEX, str(LAST_INDEX) if LAST_INDEX != -1 else 'end'),\n",
    "#                             word_match_count=count_fuzzy,\n",
    "#                             confidence=CONFIDENCE,\n",
    "#                             phrases=True,\n",
    "#                             preprocessing=False,\n",
    "                        \n",
    "#                             output_debug_path='./debug_fuzzy/EN/',\n",
    "#                             debug=True,\n",
    "#                             start_index=START_INDEX,\n",
    "#                             last_index=LAST_INDEX\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 EN <span style=\"color:red\">'fuzzy match'</span>\n",
    "\n",
    "\n",
    "#### PREPROCESSING=OFF\n",
    "# <span style=\"color:red\">FUZZY MATCH (95%)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CONFIDENCE = 95\n",
    "# START_INDEX = 0\n",
    "# LAST_INDEX = -1\n",
    "\n",
    "# filter_special_keywords(filter_list_path='./', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN/',\n",
    "#                             output_path='./reports_special/EN/',\n",
    "#                             output_file='relatorio_aproximado({0})_frases_EN_{1}-{2}.csv'\n",
    "#                                         .format(CONFIDENCE, START_INDEX, str(LAST_INDEX) if LAST_INDEX != -1 else 'end'),\n",
    "#                             word_match_count=count_fuzzy,\n",
    "#                             confidence=CONFIDENCE,\n",
    "#                             phrases=True,\n",
    "#                             preprocessing=False,\n",
    "                        \n",
    "#                             output_debug_path='./debug_fuzzy/EN/',\n",
    "#                             debug=True,\n",
    "#                             start_index=START_INDEX,\n",
    "#                             last_index=LAST_INDEX\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 PT 'exact match'\n",
    "\n",
    "\n",
    "### <span style=\"color:blue\">PREPROCESSING=OFF</span>\n",
    "### <span style=\"color:blue\">EXACT MATCH</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# filter_special_keywords(filter_list_path='./', \n",
    "#                             filter_list_file='lista_palavras_PT.txt', \n",
    "#                             raw_text_path='./raw_texts/PT/',\n",
    "                        \n",
    "#                             path_extra_stop_words='./',\n",
    "#                             file_extra_stop_words='extra_stop_words_PT.txt',\n",
    "                        \n",
    "#                             output_path='./reports_special/PT/',\n",
    "#                             output_file='relatorio_normal_frases_PT.csv',\n",
    "#                             word_match_count=count_normal,\n",
    "                        \n",
    "#                             language='portuguese',\n",
    "                        \n",
    "#                             confidence=-1,\n",
    "#                             phrases=True,\n",
    "#                             preprocessing=False\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 PT <span style=\"color:red\">'fuzzy match'</span>\n",
    "\n",
    "\n",
    "#### PREPROCESSING=OFF\n",
    "# <span style=\"color:red\">FUZZY MATCH (90%)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CONFIDENCE = 90\n",
    "# START_INDEX = 0\n",
    "# LAST_INDEX = -1\n",
    "\n",
    "# filter_special_keywords(filter_list_path='./', \n",
    "#                             filter_list_file='lista_palavras_PT.txt', \n",
    "#                             raw_text_path='./raw_texts/PT/',\n",
    "                        \n",
    "#                             path_extra_stop_words='./',\n",
    "#                             file_extra_stop_words='extra_stop_words_PT.txt',\n",
    "                        \n",
    "#                             output_path='./reports_special/PT/',\n",
    "#                             output_file='relatorio_aproximado({0})_frases_PT_{1}-{2}.csv'\n",
    "#                                         .format(CONFIDENCE, START_INDEX, str(LAST_INDEX) if LAST_INDEX != -1 else 'end'),\n",
    "#                             word_match_count=count_fuzzy,\n",
    "#                             confidence=CONFIDENCE,\n",
    "#                             phrases=True,\n",
    "#                             preprocessing=False,\n",
    "                        \n",
    "#                             language='portuguese',\n",
    "                        \n",
    "#                             output_debug_path='./debug_fuzzy/PT/',\n",
    "#                             debug=True,\n",
    "#                             start_index=START_INDEX,\n",
    "#                             last_index=LAST_INDEX\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 PT <span style=\"color:red\">'fuzzy match'</span>\n",
    "\n",
    "\n",
    "#### PREPROCESSING=OFF\n",
    "# <span style=\"color:red\">FUZZY MATCH (95%)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CONFIDENCE = 95\n",
    "# START_INDEX = 0\n",
    "# LAST_INDEX = -1\n",
    "\n",
    "# filter_special_keywords(filter_list_path='./', \n",
    "#                             filter_list_file='lista_palavras_PT.txt', \n",
    "#                             raw_text_path='./raw_texts/PT/',\n",
    "                        \n",
    "#                             path_extra_stop_words='./',\n",
    "#                             file_extra_stop_words='extra_stop_words_PT.txt',\n",
    "                        \n",
    "#                             output_path='./reports_special/PT/',\n",
    "#                             output_file='relatorio_aproximado({0})_frases_PT_{1}-{2}.csv'\n",
    "#                                         .format(CONFIDENCE, START_INDEX, str(LAST_INDEX) if LAST_INDEX != -1 else 'end'),\n",
    "#                             word_match_count=count_fuzzy,\n",
    "#                             confidence=CONFIDENCE,\n",
    "#                             phrases=True,\n",
    "#                             preprocessing=False,\n",
    "                        \n",
    "#                             language='portuguese',\n",
    "                        \n",
    "#                             output_debug_path='./debug_fuzzy/PT/',\n",
    "#                             debug=True,\n",
    "#                             start_index=START_INDEX,\n",
    "#                             last_index=LAST_INDEX\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
