{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading PDF files and basic NLP\n",
    "import PyPDF2\n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Fuzzy string match\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# To generate word clouds\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Read unicode data from the extra_stop_words file\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Frequency counting and collections\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# To navigate through all files in a directory\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define functions for the text extraction, preprocessing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://gist.github.com/boniattirodrigo/67429ada53b7337d2e79\n",
    "def remove_special_characters(word):\n",
    "\n",
    "    # Unicode normalize transforma um caracter em seu equivalente em latin.\n",
    "    nfkd = unicodedata.normalize('NFKD', word)\n",
    "    plain_word = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "\n",
    "    # return the word with only numbers, letters and spaces\n",
    "    return re.sub('[^a-zA-Z0-9 \\\\\\]', '', plain_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f\n",
    "# Updated the keywords removal \n",
    "def extract_keywords(path_pdf,\n",
    "                     pdf_file,\n",
    "                     path_output_raw_text,\n",
    "                     stop_words_language,#='portuguese', \n",
    "                     path_extra_stop_words,#='/Users/hmg/Dropbox/veve e heitor/Projeto_tese',\n",
    "                     file_extra_stop_words): #='extra_stop_words.txt'):\n",
    "    \n",
    "    # Check if the raw text is not already availabe in the path_output_raw_text\n",
    "    text = ''\n",
    "    try:\n",
    "        candidate_raw = open(os.path.join(path_output_raw_text, pdf_file.replace('.pdf', '.txt')), 'r')\n",
    "        text = candidate_raw.read()\n",
    "#         print('successfully read the raw text! ')\n",
    "    except IOError:\n",
    "#         print('gotta scan the pdf...')\n",
    "        # open allows you to read the file\n",
    "        pdfFileObj = open(os.path.join(path_pdf, pdf_file), 'rb')\n",
    "        # The pdfReader variable is a readable object that will be parsed\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        # discerning the number of pages will allow us to parse through all the pages\n",
    "        num_pages = pdfReader.numPages\n",
    "        # Just to show info about creator and creation time. \n",
    "        # print(str(pdfReader.getDocumentInfo())\n",
    "        count = 0\n",
    "\n",
    "        # The while loop will read each page\n",
    "        while count < num_pages:\n",
    "            pageObj = pdfReader.getPage(count)\n",
    "            count += 1\n",
    "            text += pageObj.extractText()\n",
    "        # This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned\n",
    "        # files.\n",
    "        if text != \"\":\n",
    "            text = text\n",
    "        # If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "        else: # fileurl\n",
    "            text = textract.process(os.path.join(path_pdf, pdf_file), method='tesseract', language='eng', encoding='utf8')\n",
    "\n",
    "        file_raw_text = open(os.path.join(path_output_raw_text, pdf_file.replace('.pdf', '.txt')), 'w', encoding='utf8')\n",
    "        file_raw_text.write(text)\n",
    "    \n",
    "    text = remove_special_characters(text)\n",
    "    # The word_tokenize() function will break our text phrases into #individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '%', '-', '.', '|', '']\n",
    "    stop_words = stopwords.words(stop_words_language)\n",
    "    extra_stop_words = open(os.path.join(path_extra_stop_words, file_extra_stop_words), 'r', encoding='utf8').read().split('\\n')\n",
    "    \n",
    "    # We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN\n",
    "    # punctuations.\n",
    "    keywords = [word.lower() for word in tokens \n",
    "                if \n",
    "                not word.lower() in stop_words and \n",
    "                not word.lower() in punctuations and \n",
    "                not word.lower() in extra_stop_words and \n",
    "                word.isalpha() and\n",
    "                len(word) >= 2]\n",
    "    \n",
    "    return np.asarray(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cloud_from_keywords_frequency(keywords_frequency, file_name, path='./wordclouds/', show_image=False):\n",
    "    wordcloud = WordCloud(width = 512, height = 512, background_color='white')\n",
    "    fig = plt.figure(figsize=(20,16),facecolor = 'white', edgecolor='blue')\n",
    "    plt.imshow(wordcloud.generate_from_frequencies(keywords_frequency), interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.savefig(path+file_name)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_statistics(keywords):\n",
    "    total_words = 0\n",
    "    keywords_dict = dict(collections.Counter(keywords).most_common())\n",
    "    for k in keywords_dict:\n",
    "        total_words += keywords_dict[k]\n",
    "\n",
    "    stats = dict()\n",
    "    for k in keywords_dict:\n",
    "        stats[k] = {'count': keywords_dict[k], 'text_frequency': keywords_dict[k]/float(total_words)}    \n",
    "#     stats['frequency'].most_common(10)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_report(path, output_file, keyword_stats):\n",
    "    output = open(path+output_file, 'w', encoding='utf8')\n",
    "    header = u'word,count,text_frequency\\n'\n",
    "    output.write(header)\n",
    "    for k in keyword_stats:\n",
    "        line = u'%s,%d,%.5f\\n' % (k, keyword_stats[k]['count'], keyword_stats[k]['text_frequency'])\n",
    "        output.write(line)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count occurrences of any word\n",
    "\n",
    "* All PDFs in a given path are processed.\n",
    "* This function generates 'raw reports', counting the occurrence and frequency of every word of the document. \n",
    "* A word is defined as a sequence of characters delimited by empty spaces, i.e. ' '. \n",
    "* **TODO:** This version of the code still doesn't handle pdfs that are in PT and EN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_PDFs(path='/Users/hmg/Desktop/data/PDF_relatorios_sustentabilidade/', \n",
    "                     raw_text_path='./raw_texts/EN/',\n",
    "                     output_path='./reports/EN/', \n",
    "                     wordclouds_path='./wordclouds/EN/',\n",
    "                     stop_words_language='english',\n",
    "                     path_extra_stop_words='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/',\n",
    "                     file_extra_stop_words='extra_stop_words_EN.txt',\n",
    "                     wordclouds=False, \n",
    "                     start_index=0):\n",
    "    # We assume there are only pdfs in this directory\n",
    "    PDFs = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    PDFs = list(filter(lambda pdf: pdf.find('.pdf') != -1, PDFs))\n",
    "        \n",
    "    counter = start_index\n",
    "    for pdf in PDFs[counter:]:\n",
    "        \n",
    "        print('Processing PDFs ({0}) {1} of {2} ({3})'.format(stop_words_language, counter+1, len(PDFs), pdf))\n",
    "        keywords = extract_keywords(path, pdf, \n",
    "                                    path_output_raw_text=raw_text_path, \n",
    "                                    stop_words_language=stop_words_language,\n",
    "                                    path_extra_stop_words=path_extra_stop_words,\n",
    "                                    file_extra_stop_words=file_extra_stop_words)\n",
    "        keywords_statistics = calculate_statistics(keywords)\n",
    "        write_report(output_path, pdf.replace('.pdf', '.csv'), keywords_statistics)\n",
    "        counter = counter + 1\n",
    "        if wordclouds:\n",
    "            word_cloud_from_keywords_frequency(collections.Counter(keywords), \n",
    "                                               pdf.replace('.pdf', '.png'), \n",
    "                                               path=wordclouds_path,\n",
    "                                               show_image=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENglish PDFs processing\n",
    "# process_PDFs(path='/Users/hmg/Desktop/Data/pdfs_EN/', \n",
    "#              raw_text_path='./raw_texts/EN/',\n",
    "#              output_path='./reports/EN/', \n",
    "#              wordclouds_path='./wordclouds/EN/',\n",
    "#              stop_words_language='english',\n",
    "#              wordclouds=True,\n",
    "#             start_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PorTuguese PDFs processing\n",
    "# for i in range(1,5): # Hardcoded 5 because I know there are only 4 directories... \n",
    "#     print('Processing pdfs_PT{0}'.format(i))\n",
    "#     process_PDFs(path='/Users/hmg/Desktop/Data/pdfs_PT{0}/'.format(i), \n",
    "#              raw_text_path='./raw_texts/PT/',\n",
    "#              output_path='./reports/PT/', \n",
    "#              wordclouds_path='./wordclouds/PT/',\n",
    "#              stop_words_language='portuguese',\n",
    "#              file_extra_stop_words='extra_stop_words_PT.txt',\n",
    "#              wordclouds=True,\n",
    "#              start_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Count occurrences of predefined phrases/words\n",
    "\n",
    "**IN-DEVELOPMENT:** Count occurrences of predefined text using 'fuzzy string match', added ```word_match_count``` lambda object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def count_total_words(text, delim=' '): \n",
    "#     return len(remove_special_characters(text).split(delim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_normal(text, word, confidence = 0, phrases = False, debug = False, output = ''):\n",
    "    if phrases:\n",
    "        return text.count(word)\n",
    "    else:\n",
    "        counter = 0\n",
    "        for p in text.split(' '):\n",
    "            if(p == word):\n",
    "                counter = counter + 1\n",
    "        return counter\n",
    "\n",
    "def count_fuzzy(text, word, confidence = 95, phrases=False, debug = False, output = ''):\n",
    "    counter = 0\n",
    "    if len(word) == 0:\n",
    "        return 0\n",
    "    # Phrases\n",
    "    if phrases:\n",
    "        # Sliding window strategy: create a text_word from text with length word.len \n",
    "        #  and by moving 1 character at a time\n",
    "        #     If match, then skip the next word.len, just to avoid double counting!\n",
    "        i = 0\n",
    "        ## DEBUG\n",
    "#         print('count_fuzzy - word = {0} and, len(text) = {1}, confidence = {2}'.format(word, len(text), confidence))\n",
    "        if debug:\n",
    "            output.write('{0} ({1})'.format(word, confidence) + ',')\n",
    "#             print('{0} ({1})'.format(word, confidence))\n",
    "    \n",
    "        while i < len(text):\n",
    "            text_word_last_idx = i+len(word)\n",
    "            match_confidence = 0\n",
    "            if text_word_last_idx <= len(text):\n",
    "                match_confidence = fuzz.ratio(text[i:text_word_last_idx], word)\n",
    "#                 print('token_set_ratio({0},{1})={2}'.format(text[i:text_word_last_idx], word, match_confidence))\n",
    "                if match_confidence > confidence:\n",
    "                    counter += 1\n",
    "                    if debug:\n",
    "                        output.write('{0} ({1})'.format(text[i:text_word_last_idx], match_confidence) + ',')\n",
    "#                         print('{0} =~ {1} ({2})'.format(text[i:text_word_last_idx], word, match_confidence))\n",
    "                    i += len(word)\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                break\n",
    "            ## DEBUG - only about 100 characters\n",
    "#             if debug and (i % int(len(text)/10) == 0):\n",
    "#                 print('{} of {} = {:.2f}%'.format(i, len(text), i/len(text)*100))\n",
    "    else:\n",
    "        text_vec = text.split()\n",
    "        if debug:\n",
    "            output.write('{0} ({1})'.format(word, confidence) + ',')\n",
    "        for w in text_vec:\n",
    "            match_confidence = fuzz.ratio(w, word)\n",
    "            if match_confidence > confidence:\n",
    "                if debug:\n",
    "                    output.write('{0} ({1})'.format(w, match_confidence) + ',')\n",
    "                counter += 1\n",
    "    if debug:\n",
    "        output.write('\\n')\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "                            filter_list_file='lista_palavras_EN.txt', \n",
    "                            raw_text_path='./raw_texts/EN/',\n",
    "                            output_path='./reports_special/EN/',\n",
    "                            output_file='output_EN.csv',\n",
    "                            stop_words_language='english',\n",
    "                            path_extra_stop_words='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/',\n",
    "                            file_extra_stop_words='extra_stop_words_EN.txt',\n",
    "                            word_match_count=count_normal,\n",
    "                            confidence=95,\n",
    "                            phrases=False,\n",
    "                            output_debug_path='./debug_fuzzy/EN/',\n",
    "                            debug=False,\n",
    "                            start_index=0,\n",
    "                            last_index=-1):\n",
    "    print('Starting filter_special_keywords')\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '%', '-', '.', '|', '']\n",
    "    stop_words = stopwords.words(stop_words_language)\n",
    "    extra_stop_words = open(os.path.join(path_extra_stop_words, file_extra_stop_words), 'r', encoding='utf8').read().split('\\n')\n",
    "    \n",
    "    if not phrases:\n",
    "        print('[not phrases] Stopwords will be removed prior to processing the texts.')\n",
    "        \n",
    "    \n",
    "    filter_words = ''\n",
    "    try:\n",
    "        filter_words = open(os.path.join(filter_list_path, filter_list_file), 'r', encoding='utf8').read().split('\\n')\n",
    "        print('successfully read the filter words at {0} named {1}'.format(filter_list_path, filter_list_file))\n",
    "    except IOError:\n",
    "        print('failed to read the filter words at {0} named {1}'.format(filter_list_path, filter_list_file))\n",
    "    \n",
    "    # assuming there are only the raw texts in the directory\n",
    "    text_files = [f for f in listdir(raw_text_path) if isfile(join(raw_text_path, f))]\n",
    "    text_files = list(filter(lambda text_file: text_file.find('.txt') != -1, text_files))\n",
    "    \n",
    "    stats_per_file = {}\n",
    "    total_words_per_file = {}\n",
    "    \n",
    "    processed = start_index\n",
    "    if last_index == -1:\n",
    "        last_index = len(text_files)\n",
    "    \n",
    "    for text_file in text_files[processed:last_index]:\n",
    "        print('Processing Text ({0}) {1} of {2} ({3})'.format(stop_words_language, processed+1, \n",
    "                                                              last_index, text_file))\n",
    "        debug_output = open(output_debug_path+'DEBUG_'+text_file.replace('.txt','.csv'), 'w', encoding='utf8') \n",
    "        try:\n",
    "            text = open(os.path.join(raw_text_path, text_file), 'r').read()\n",
    "        except IOError:\n",
    "            print('Failed to open file at {0} named {1}'.format(raw_text_path, text_file))\n",
    "                        \n",
    "        # Transform the original text, remove special characters and set it to lower\n",
    "        search_text = remove_special_characters(text).lower()\n",
    "        \n",
    "        # TODO: merge this and the previous stop_word filter in a function. \n",
    "        search_text_nostopwords = [word for word in search_text.split(' ') \n",
    "                                   if \n",
    "                                   not word in stop_words and\n",
    "                                   not word in punctuations and \n",
    "                                   not word in extra_stop_words and\n",
    "                                   len(word) >= 1]\n",
    "        \n",
    "        stats = {}\n",
    "        stats['@TOTAL_WORDS'] = len(search_text_nostopwords)\n",
    "        if not phrases:\n",
    "            search_text = ' '.join(search_text_nostopwords)\n",
    "        \n",
    "        \n",
    "        for filter_word in filter_words:\n",
    "            # Transform the filter_word, remove special characters and set it to lower\n",
    "            search_word = remove_special_characters(filter_word).lower()\n",
    "            stats[filter_word] = word_match_count(search_text, search_word, confidence, phrases, debug, debug_output)\n",
    "            # text.lower().count(filter_word.lower())\n",
    "#             line = u'%s,%d,%.5f\\n' % (filter_word, counter, -1)\n",
    "\n",
    "        stats_per_file[text_file.replace('.txt','')] = stats\n",
    "        \n",
    "        processed = processed + 1\n",
    "        \n",
    "    output = open(output_path+output_file, 'w', encoding='utf8')   \n",
    "    header = u',' + u','.join(text_files)+u'\\n'\n",
    "    header = header + u'words \\ total_words_per_file,' + u','.join(total_words_per_file)\n",
    "    output.write(header)\n",
    "    \n",
    "    for text_file in text_files[start_index:last_index]:\n",
    "        output.write(str(stats_per_file[text_file.replace('.txt','')]['@TOTAL_WORDS']) + ',')\n",
    "    output.write('\\n')\n",
    "    \n",
    "    for filter_word in filter_words:\n",
    "        output.write(filter_word + ',')\n",
    "        for text_file in text_files[start_index:last_index]:\n",
    "#             line = u'%s,%d,%.5f\\n' % (filter_word, counter, -1)\n",
    "            output.write(str(stats_per_file[text_file.replace('.txt','')][filter_word]) + ',')\n",
    "        output.write('\\n')\n",
    "    output.close()\n",
    "    print('Finishing filter_special_keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Filter the special words EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN/',\n",
    "#                             output_path='./reports_special/EN/',\n",
    "#                             output_file='relatorio_normal_frases_EN.csv',\n",
    "#                             word_match_count=count_normal,\n",
    "#                             confidence=-1,\n",
    "#                             phrases=True\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting filter_special_keywords\n",
      "successfully read the filter words at /Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/ named lista_palavras_EN.txt\n",
      "Processing Text (english) 1 of 2 (CPFL_RE_10_2014_EN.txt)\n",
      "Processing Text (english) 2 of 2 (ESTC_11_2015_EN.txt)\n",
      "Finishing filter_special_keywords\n",
      "CPU times: user 52.8 s, sys: 231 ms, total: 53 s\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## APROXIMADO 95%\n",
    "confidence = 95\n",
    "start_index = 0\n",
    "last_index = 75\n",
    "filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "                            filter_list_file='lista_palavras_EN.txt', \n",
    "                            raw_text_path='./raw_texts/EN/',\n",
    "                            output_path='./reports_special/EN/',\n",
    "                            output_file='relatorio_aproximado({0})_frases_EN_{1}-{2}.csv'\n",
    "                                    .format(confidence, start_index, last_index),\n",
    "                            word_match_count=count_fuzzy,\n",
    "                            confidence=confidence,\n",
    "                            phrases=True,\n",
    "                            debug=True,\n",
    "                            output_debug_path='./debug_fuzzy/EN/',\n",
    "                            start_index=start_index,\n",
    "                            last_index=last_index\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting filter_special_keywords\n",
      "successfully read the filter words at /Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/ named lista_palavras_EN.txt\n",
      "Processing Text (english) 3 of 4 (BUN_8_Non_2012_EN.txt)\n",
      "Processing Text (english) 4 of 4 (EMBR_19_2012_EN.txt)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## APROXIMADO 95%\n",
    "confidence = 95\n",
    "start_index = 76\n",
    "last_index = 151\n",
    "filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "                            filter_list_file='lista_palavras_EN.txt', \n",
    "                            raw_text_path='./raw_texts/EN/',\n",
    "                            output_path='./reports_special/EN/',\n",
    "                            output_file='relatorio_aproximado({0})_frases_EN_{1}-{2}.csv'\n",
    "                                    .format(confidence, start_index, last_index),\n",
    "                            word_match_count=count_fuzzy,\n",
    "                            confidence=confidence,\n",
    "                            phrases=True,\n",
    "                            debug=True,\n",
    "                            output_debug_path='./debug_fuzzy/EN/',\n",
    "                            start_index=start_index,\n",
    "                            last_index=last_index\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(len(PDFs))\n",
    "# print(len(PDFs_PT))\n",
    "# print(len(PDFs_EN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## APROXIMADO 85%\n",
    "# confidence = 85\n",
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN_dev/',\n",
    "#                             output_path='./reports_special/EN/',\n",
    "#                             output_file='relatorio_aproximado({0})_frases_EN_dev.csv'.format(confidence),\n",
    "#                             word_match_count=count_fuzzy,\n",
    "#                             confidence=confidence,\n",
    "#                             phrases=True,\n",
    "#                             debug=True,\n",
    "#                             output_debug_path='./debug_fuzzy/EN_dev/'\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = u'administrativa da joao Elekeiroz. GRI 2.3  joao| 2.4As vendas para o mercado interno 88 corresponderam a 88% da Receita Líquida da Elekeiroz em 2011. As exportações para\\nmais de 30 países responderam pelos restantes 12%. Os produtos \\norgânicos foram vendidos nos mercados interno e externo, enquanto os inorgânicos tiveram como destino o mercado interno. MISSÃO GRI 4.8  GRI 2.2 | 2.7 | 2.8 Buscar a melhoria contínua dos produtos e serviços oferecidos aos seus clientes internos e \\nexternos; destacar-se em '\n",
    "print('count = {0}'.format(count_total_words(text)))\n",
    "\n",
    "count_normal(text, 'joão', phrases=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keywords = extract_keywords('/Users/hmg/Downloads/', '3M_RS2014.pdf', '/Users/hmg/Downloads/')\n",
    "# keywords_frequency = collections.Counter(keywords)\n",
    "# \n",
    "# keywords_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keywords = extract_keywords('/Users/hmg/Desktop/data/PDF_relatorios_sustentabilidade/FIBR_7_2012_EN.pdf', \n",
    "#                             stop_words_language='english')\n",
    "# keywords_frequency = collections.Counter(keywords)\n",
    "\n",
    "# word_cloud_from_keywords_frequency(keywords_frequency, file_name='hey.png', plot=False)\n",
    "\n",
    "# keywords_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate_statistics(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testCount = 3 and phraseCount = 2\n"
     ]
    }
   ],
   "source": [
    "text = 'test text with test words in the text test words'\n",
    "\n",
    "testCount = count_normal(text, 'test', phrases = False)\n",
    "phraseCount = count_normal(text, 'test words', phrases = True)\n",
    "print('testCount = {0} and phraseCount = {1}'.format(testCount, phraseCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Projeto_tese/', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN/',\n",
    "#                             output_path='./reports_special/EN/')\n",
    "\n",
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Projeto_tese/', \n",
    "filter_special_keywords(filter_list_path='./', \n",
    "                            filter_list_file='lista_frases_v3_dev.txt', \n",
    "                            raw_text_path='./raw_texts/PT_dev/',\n",
    "                            output_path='./reports_special/PT/',\n",
    "                            output_file='output_PT_dev_fuzzy_phrases95.csv',\n",
    "                            word_match_count=count_fuzzy,\n",
    "                            confidence=95,\n",
    "                            phrases=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests with fuzzy string match\n",
    "# fuzz.ratio(str.lower(u'Casarão'), str.lower(u'Caserão'))\n",
    "# fuzz.token_set_ratio(str.lower(u'Casarão'), str.lower(u'Casârão'))\n",
    "\n",
    "# normal_count = count_normal(\"mama mia this is a mama and there is no mia in this mama\", \"mãma\")\n",
    "# fuzzy_count = count_fuzzy(\"mama mia this is a mama and there is no mia in this mama\", \"mãma\", 80)\n",
    "\n",
    "# print('normal = {0}, fuzzy = {1}'.format(normal_count, fuzzy_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(str.lower(u'vacao da biodiversidade'), str.lower(u''))\n",
    "# fuzz.token_set_ratio(str.lower(u'EN-13'), str.lower(u'EN-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
