{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading PDF files and basic NLP\n",
    "import PyPDF2\n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Fuzzy string match\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# To generate word clouds\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Read unicode data from the extra_stop_words file\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Frequency counting and collections\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# To navigate through all files in a directory\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define functions for the text extraction, preprocessing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://gist.github.com/boniattirodrigo/67429ada53b7337d2e79\n",
    "def remove_special_characters(word):\n",
    "\n",
    "    # Unicode normalize transforma um caracter em seu equivalente em latin.\n",
    "    nfkd = unicodedata.normalize('NFKD', word)\n",
    "    plain_word = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "\n",
    "    # return the word with only numbers, letters and spaces\n",
    "    return re.sub('[^a-zA-Z0-9 \\\\\\]', '', plain_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f\n",
    "# Updated the keywords removal \n",
    "def extract_keywords(path_pdf,\n",
    "                     pdf_file,\n",
    "                     path_output_raw_text,\n",
    "                     stop_words_language,#='portuguese', \n",
    "                     path_extra_stop_words,#='/Users/hmg/Dropbox/veve e heitor/Projeto_tese',\n",
    "                     file_extra_stop_words): #='extra_stop_words.txt'):\n",
    "    \n",
    "    # Check if the raw text is not already availabe in the path_output_raw_text\n",
    "    text = ''\n",
    "    try:\n",
    "        candidate_raw = open(os.path.join(path_output_raw_text, pdf_file.replace('.pdf', '.txt')), 'r')\n",
    "        text = candidate_raw.read()\n",
    "#         print('successfully read the raw text! ')\n",
    "    except IOError:\n",
    "#         print('gotta scan the pdf...')\n",
    "        # open allows you to read the file\n",
    "        pdfFileObj = open(os.path.join(path_pdf, pdf_file), 'rb')\n",
    "        # The pdfReader variable is a readable object that will be parsed\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        # discerning the number of pages will allow us to parse through all the pages\n",
    "        num_pages = pdfReader.numPages\n",
    "        # Just to show info about creator and creation time. \n",
    "        # print(str(pdfReader.getDocumentInfo())\n",
    "        count = 0\n",
    "\n",
    "        # The while loop will read each page\n",
    "        while count < num_pages:\n",
    "            pageObj = pdfReader.getPage(count)\n",
    "            count += 1\n",
    "            text += pageObj.extractText()\n",
    "        # This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned\n",
    "        # files.\n",
    "        if text != \"\":\n",
    "            text = text\n",
    "        # If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "        else: # fileurl\n",
    "            text = textract.process(os.path.join(path_pdf, pdf_file), method='tesseract', language='eng', encoding='utf8')\n",
    "\n",
    "        file_raw_text = open(os.path.join(path_output_raw_text, pdf_file.replace('.pdf', '.txt')), 'w', encoding='utf8')\n",
    "        file_raw_text.write(text)\n",
    "    \n",
    "    text = remove_special_characters(text)\n",
    "    # The word_tokenize() function will break our text phrases into #individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '%', '-', '.', '|', '']\n",
    "    stop_words = stopwords.words(stop_words_language)\n",
    "    extra_stop_words = open(os.path.join(path_extra_stop_words, file_extra_stop_words), 'r', encoding='utf8').read().split('\\n')\n",
    "    \n",
    "    # We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN\n",
    "    # punctuations.\n",
    "    keywords = [word.lower() for word in tokens \n",
    "                if \n",
    "                not word.lower() in stop_words and \n",
    "                not word.lower() in punctuations and \n",
    "                not word.lower() in extra_stop_words and \n",
    "                word.isalpha() and\n",
    "                len(word) >= 2]\n",
    "    \n",
    "    return np.asarray(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cloud_from_keywords_frequency(keywords_frequency, file_name, path='./wordclouds/', show_image=False):\n",
    "    wordcloud = WordCloud(width = 512, height = 512, background_color='white')\n",
    "    fig = plt.figure(figsize=(20,16),facecolor = 'white', edgecolor='blue')\n",
    "    plt.imshow(wordcloud.generate_from_frequencies(keywords_frequency), interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.savefig(path+file_name)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_statistics(keywords):\n",
    "    total_words = 0\n",
    "    keywords_dict = dict(collections.Counter(keywords).most_common())\n",
    "    for k in keywords_dict:\n",
    "        total_words += keywords_dict[k]\n",
    "\n",
    "    stats = dict()\n",
    "    for k in keywords_dict:\n",
    "        stats[k] = {'count': keywords_dict[k], 'text_frequency': keywords_dict[k]/float(total_words)}    \n",
    "#     stats['frequency'].most_common(10)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_report(path, output_file, keyword_stats):\n",
    "    output = open(path+output_file, 'w', encoding='utf8')\n",
    "    header = u'word,count,text_frequency\\n'\n",
    "    output.write(header)\n",
    "    for k in keyword_stats:\n",
    "        line = u'%s,%d,%.5f\\n' % (k, keyword_stats[k]['count'], keyword_stats[k]['text_frequency'])\n",
    "        output.write(line)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count occurrences of any word\n",
    "\n",
    "* All PDFs in a given path are processed.\n",
    "* This function generates 'raw reports', counting the occurrence and frequency of every word of the document. \n",
    "* A word is defined as a sequence of characters delimited by empty spaces, i.e. ' '. \n",
    "* **TODO:** This version of the code still doesn't handle pdfs that are in PT and EN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_PDFs(path='/Users/hmg/Desktop/data/PDF_relatorios_sustentabilidade/', \n",
    "                     raw_text_path='./raw_texts/EN/',\n",
    "                     output_path='./reports/EN/', \n",
    "                     wordclouds_path='./wordclouds/EN/',\n",
    "                     stop_words_language='english',\n",
    "                     path_extra_stop_words='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/',\n",
    "                     file_extra_stop_words='extra_stop_words_EN.txt',\n",
    "                     wordclouds=False, \n",
    "                     start_index=0):\n",
    "    # We assume there are only pdfs in this directory\n",
    "    PDFs = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    PDFs = list(filter(lambda pdf: pdf.find('.pdf') != -1, PDFs))\n",
    "        \n",
    "    counter = start_index\n",
    "    for pdf in PDFs[counter:]:\n",
    "        \n",
    "        print('Processing PDFs ({0}) {1} of {2} ({3})'.format(stop_words_language, counter+1, len(PDFs), pdf))\n",
    "        keywords = extract_keywords(path, pdf, \n",
    "                                    path_output_raw_text=raw_text_path, \n",
    "                                    stop_words_language=stop_words_language,\n",
    "                                    path_extra_stop_words=path_extra_stop_words,\n",
    "                                    file_extra_stop_words=file_extra_stop_words)\n",
    "        keywords_statistics = calculate_statistics(keywords)\n",
    "        write_report(output_path, pdf.replace('.pdf', '.csv'), keywords_statistics)\n",
    "        counter = counter + 1\n",
    "        if wordclouds:\n",
    "            word_cloud_from_keywords_frequency(collections.Counter(keywords), \n",
    "                                               pdf.replace('.pdf', '.png'), \n",
    "                                               path=wordclouds_path,\n",
    "                                               show_image=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENglish PDFs processing\n",
    "# process_PDFs(path='/Users/hmg/Desktop/Data/pdfs_EN/', \n",
    "#              raw_text_path='./raw_texts/EN/',\n",
    "#              output_path='./reports/EN/', \n",
    "#              wordclouds_path='./wordclouds/EN/',\n",
    "#              stop_words_language='english',\n",
    "#              wordclouds=True,\n",
    "#             start_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PorTuguese PDFs processing\n",
    "# for i in range(1,5): # Hardcoded 5 because I know there are only 4 directories... \n",
    "#     print('Processing pdfs_PT{0}'.format(i))\n",
    "#     process_PDFs(path='/Users/hmg/Desktop/Data/pdfs_PT{0}/'.format(i), \n",
    "#              raw_text_path='./raw_texts/PT/',\n",
    "#              output_path='./reports/PT/', \n",
    "#              wordclouds_path='./wordclouds/PT/',\n",
    "#              stop_words_language='portuguese',\n",
    "#              file_extra_stop_words='extra_stop_words_PT.txt',\n",
    "#              wordclouds=True,\n",
    "#              start_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Count occurrences of predefined phrases/words\n",
    "\n",
    "**IN-DEVELOPMENT:** Count occurrences of predefined text using 'fuzzy string match', added ```word_match_count``` lambda object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def count_total_words(text, delim=' '): \n",
    "#     return len(remove_special_characters(text).split(delim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_normal(text, word, confidence = 0, phrases = False, debug = False, output = ''):\n",
    "    if phrases:\n",
    "        return text.count(word)\n",
    "    else:\n",
    "        counter = 0\n",
    "        for p in text.split(' '):\n",
    "            if(p == word):\n",
    "                counter = counter + 1\n",
    "        return counter\n",
    "\n",
    "def count_fuzzy(text, word, confidence = 95, phrases=False, debug = False, output = ''):\n",
    "    counter = 0\n",
    "    if len(word) == 0:\n",
    "        return 0\n",
    "    # Phrases\n",
    "    if phrases:\n",
    "        # Sliding window strategy: create a text_word from text with length word.len \n",
    "        #  and by moving 1 character at a time\n",
    "        #     If match, then skip the next word.len, just to avoid double counting!\n",
    "        i = 0\n",
    "        ## DEBUG\n",
    "#         print('count_fuzzy - word = {0} and, len(text) = {1}, confidence = {2}'.format(word, len(text), confidence))\n",
    "        if debug:\n",
    "            output.write('{0} ({1})'.format(word, confidence) + ',')\n",
    "#             print('{0} ({1})'.format(word, confidence))\n",
    "    \n",
    "        while i < len(text):\n",
    "            text_word_last_idx = i+len(word)\n",
    "            match_confidence = 0\n",
    "            if text_word_last_idx <= len(text):\n",
    "                match_confidence = fuzz.ratio(text[i:text_word_last_idx], word)\n",
    "#                 print('token_set_ratio({0},{1})={2}'.format(text[i:text_word_last_idx], word, match_confidence))\n",
    "                if match_confidence > confidence:\n",
    "                    counter += 1\n",
    "                    if debug:\n",
    "                        output.write('{0} ({1})'.format(text[i:text_word_last_idx], match_confidence) + ',')\n",
    "#                         print('{0} =~ {1} ({2})'.format(text[i:text_word_last_idx], word, match_confidence))\n",
    "                    i += len(word)\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                break\n",
    "            ## DEBUG - only about 100 characters\n",
    "#             if debug and (i % int(len(text)/10) == 0):\n",
    "#                 print('{} of {} = {:.2f}%'.format(i, len(text), i/len(text)*100))\n",
    "    else:\n",
    "        text_vec = text.split()\n",
    "        if debug:\n",
    "            output.write('{0} ({1})'.format(word, confidence) + ',')\n",
    "        for w in text_vec:\n",
    "            match_confidence = fuzz.ratio(w, word)\n",
    "            if match_confidence > confidence:\n",
    "                if debug:\n",
    "                    output.write('{0} ({1})'.format(w, match_confidence) + ',')\n",
    "                counter += 1\n",
    "    if debug:\n",
    "        output.write('\\n')\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "                            filter_list_file='lista_palavras_EN.txt', \n",
    "                            raw_text_path='./raw_texts/EN/',\n",
    "                            output_path='./reports_special/EN/',\n",
    "                            output_file='output_EN.csv',\n",
    "                            stop_words_language='english',\n",
    "                            path_extra_stop_words='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/',\n",
    "                            file_extra_stop_words='extra_stop_words_EN.txt',\n",
    "                            word_match_count=count_normal,\n",
    "                            confidence=95,\n",
    "                            phrases=False,\n",
    "                            output_debug_path='./debug_fuzzy/EN/',\n",
    "                            debug=False):\n",
    "    print('Starting filter_special_keywords')\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '%', '-', '.', '|', '']\n",
    "    stop_words = stopwords.words(stop_words_language)\n",
    "    extra_stop_words = open(os.path.join(path_extra_stop_words, file_extra_stop_words), 'r', encoding='utf8').read().split('\\n')\n",
    "    \n",
    "    if not phrases:\n",
    "        print('[not phrases] Stopwords will be removed prior to processing the texts.')\n",
    "        \n",
    "    \n",
    "    filter_words = ''\n",
    "    try:\n",
    "        filter_words = open(os.path.join(filter_list_path, filter_list_file), 'r', encoding='utf8').read().split('\\n')\n",
    "        print('successfully read the filter words at {0} named {1}'.format(filter_list_path, filter_list_file))\n",
    "    except IOError:\n",
    "        print('failed to read the filter words at {0} named {1}'.format(filter_list_path, filter_list_file))\n",
    "    \n",
    "    # assuming there are only the raw texts in the directory\n",
    "    text_files = [f for f in listdir(raw_text_path) if isfile(join(raw_text_path, f))]\n",
    "    text_files = list(filter(lambda text_file: text_file.find('.txt') != -1, text_files))\n",
    "    \n",
    "    stats_per_file = {}\n",
    "    total_words_per_file = {}\n",
    "    processed = 0\n",
    "    for text_file in text_files:\n",
    "        print('Processing Text ({0}) {1} of {2} ({3})'.format(stop_words_language, processed+1, \n",
    "                                                              len(text_files), text_file))\n",
    "        debug_output = open(output_debug_path+'DEBUG_'+text_file.replace('.txt','.csv'), 'w', encoding='utf8') \n",
    "        try:\n",
    "            text = open(os.path.join(raw_text_path, text_file), 'r').read()\n",
    "        except IOError:\n",
    "            print('Failed to open file at {0} named {1}'.format(raw_text_path, text_file))\n",
    "                        \n",
    "        # Transform the original text, remove special characters and set it to lower\n",
    "        search_text = remove_special_characters(text).lower()\n",
    "        \n",
    "        # TODO: merge this and the previous stop_word filter in a function. \n",
    "        search_text_nostopwords = [word for word in search_text.split(' ') \n",
    "                                   if \n",
    "                                   not word in stop_words and\n",
    "                                   not word in punctuations and \n",
    "                                   not word in extra_stop_words and\n",
    "                                   len(word) >= 1]\n",
    "        \n",
    "        stats = {}\n",
    "        stats['@TOTAL_WORDS'] = len(search_text_nostopwords)\n",
    "        if not phrases:\n",
    "            search_text = ' '.join(search_text_nostopwords)\n",
    "        \n",
    "        \n",
    "        for filter_word in filter_words:\n",
    "            # Transform the filter_word, remove special characters and set it to lower\n",
    "            search_word = remove_special_characters(filter_word).lower()\n",
    "            stats[filter_word] = word_match_count(search_text, search_word, confidence, phrases, debug, debug_output)\n",
    "            # text.lower().count(filter_word.lower())\n",
    "#             line = u'%s,%d,%.5f\\n' % (filter_word, counter, -1)\n",
    "\n",
    "        stats_per_file[text_file.replace('.txt','')] = stats\n",
    "        \n",
    "        processed = processed + 1\n",
    "        \n",
    "    output = open(output_path+output_file, 'w', encoding='utf8')   \n",
    "    header = u',' + u','.join(text_files)+u'\\n'\n",
    "    header = header + u'words \\ total_words_per_file,' + u','.join(total_words_per_file)\n",
    "    output.write(header)\n",
    "    \n",
    "    for text_file in text_files:\n",
    "        output.write(str(stats_per_file[text_file.replace('.txt','')]['@TOTAL_WORDS']) + ',')\n",
    "    output.write('\\n')\n",
    "    \n",
    "    for filter_word in filter_words:\n",
    "        output.write(filter_word + ',')\n",
    "        for text_file in text_files:\n",
    "#             line = u'%s,%d,%.5f\\n' % (filter_word, counter, -1)\n",
    "            output.write(str(stats_per_file[text_file.replace('.txt','')][filter_word]) + ',')\n",
    "        output.write('\\n')\n",
    "    output.close()\n",
    "    print('Finishing filter_special_keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Filter the special words EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting filter_special_keywords\n",
      "successfully read the filter words at /Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/ named lista_palavras_EN.txt\n",
      "Processing Text (english) 1 of 209 (CPFL_RE_10_2014_EN.txt)\n",
      "Processing Text (english) 2 of 209 (ESTC_11_2015_EN.txt)\n",
      "Processing Text (english) 3 of 209 (BUN_8_Non_2012_EN.txt)\n",
      "Processing Text (english) 4 of 209 (EMBR_19_2012_EN.txt)\n",
      "Processing Text (english) 5 of 209 (LREN_17_2015_EN.txt)\n",
      "Processing Text (english) 6 of 209 (Syngenta_24_2015_EN.txt)\n",
      "Processing Text (english) 7 of 209 (TCSA_21_2017_EN.txt)\n",
      "Processing Text (english) 8 of 209 (ALI_14_Non_2015_EN.txt)\n",
      "Processing Text (english) 9 of 209 (END_2_Non_2011_EN.txt)\n",
      "Processing Text (english) 10 of 209 (FIBR_7_2014_EN.txt)\n",
      "Processing Text (english) 11 of 209 (SANB_14_2011_EN.txt)\n",
      "Processing Text (english) 12 of 209 (IMA_33_Non_2013_EN.txt)\n",
      "Processing Text (english) 13 of 209 (CIEL_14_2016_EN.txt)\n",
      "Processing Text (english) 14 of 209 (INP_24_Non_2013_EN.txt)\n",
      "Processing Text (english) 15 of 209 (INS_32_Non_2017.txt)\n",
      "Processing Text (english) 16 of 209 (‎BRKM_16_2012_EN.txt)\n",
      "Processing Text (english) 17 of 209 (POR_5_Non_2013_EN.txt)\n",
      "Processing Text (english) 18 of 209 (TIMP_4_2014_EN.txt)\n",
      "Processing Text (english) 19 of 209 (DTEX_7_2013_EN.txt)\n",
      "Processing Text (english) 20 of 209 (AMA_12_Non_2015_EN.txt)\n",
      "Processing Text (english) 21 of 209 (CPLE_2_2014_EN.txt)\n",
      "Processing Text (english) 22 of 209 (USI_10_Non_2012_EN.txt)\n",
      "Processing Text (english) 23 of 209 (KLBN_7_2014_EN.txt)\n",
      "Processing Text (english) 24 of 209 (ITSA_1_2014_EN.txt)\n",
      "Processing Text (english) 25 of 209 (ABEV_8_2016_EN.txt)\n",
      "Processing Text (english) 26 of 209 (VOT_20_Non_2015_EN.txt)\n",
      "Processing Text (english) 27 of 209 (BBAS_14_2016_EN.txt)\n",
      "Processing Text (english) 28 of 209 (GRU_12_Non_2016_EN.txt)\n",
      "Processing Text (english) 29 of 209 (B3SA_14_2016_EN.txt)\n",
      "Processing Text (english) 30 of 209 (CESP_2_2017_EN.txt)\n",
      "Processing Text (english) 31 of 209 (SUZ_7_Non_2011_EN.txt)\n",
      "Processing Text (english) 32 of 209 (Givaudan_16_2013_EN.txt)\n",
      "Processing Text (english) 33 of 209 (CPLE_2_2016_EN.txt)\n",
      "Processing Text (english) 34 of 209 (EGIE_10_2014_EN.txt)\n",
      "Processing Text (english) 35 of 209 (EUR_16_Non_2015_EN.txt)\n",
      "Processing Text (english) 36 of 209 (EMB_6_Non_2014_EN.txt)\n",
      "Processing Text (english) 37 of 209 (LAME_25_2016_EN.txt)\n",
      "Processing Text (english) 38 of 209 (ELEK_16_2013_EN.txt)\n",
      "Processing Text (english) 39 of 209 (CESP_2_2015_EN.txt)\n",
      "Processing Text (english) 40 of 209 (WHRL_1_2012_EN.txt)\n",
      "Processing Text (english) 41 of 209 (BBAS_14_2014_EN.txt)\n",
      "Processing Text (english) 42 of 209 (DOW_16_Non_2013_EN.txt)\n",
      "Processing Text (english) 43 of 209 (ROMI_23_2012_EN.txt)\n",
      "Processing Text (english) 44 of 209 (Syngenta_24_2017_EN.txt)\n",
      "Processing Text (english) 45 of 209 (FDC_11_Non_2014.txt)\n",
      "Processing Text (english) 46 of 209 (BRFS_8_2015_EN.txt)\n",
      "Processing Text (english) 47 of 209 (VALE_26_2014_EN.txt)\n",
      "Processing Text (english) 48 of 209 (CCRO_1_2011_EN.txt)\n",
      "Processing Text (english) 49 of 209 (C_A_25_Non_2012_EN.txt)\n",
      "Processing Text (english) 50 of 209 (PRE_14_Non_2014_EN.txt)\n",
      "Processing Text (english) 51 of 209 (EDP_2_Non_2013_EN.txt)\n",
      "Processing Text (english) 52 of 209 (BEEF_8_2014_EN.txt)\n",
      "Processing Text (english) 53 of 209 (BRSR_14_2012_EN.txt)\n",
      "Processing Text (english) 54 of 209 (USI_10_Non_2014_EN.txt)\n",
      "Processing Text (english) 55 of 209 (V_M_3_Non_2013_EN.txt)\n",
      "Processing Text (english) 56 of 209 (NATU_15_2013_EN.txt)\n",
      "Processing Text (english) 57 of 209 (CMIG_2_2013_EN.txt)\n",
      "Processing Text (english) 58 of 209 (ARMT_3_2015_EN.txt)\n",
      "Processing Text (english) 59 of 209 (POR_5_Non_2015_EN.txt)\n",
      "Processing Text (english) 60 of 209 (DTEX_7_2015_EN.txt)\n",
      "Processing Text (english) 61 of 209 (VER_7_Non_2011_EN.txt)\n",
      "Processing Text (english) 62 of 209 (CAR_24_Non_2016_EN.txt)\n",
      "Processing Text (english) 63 of 209 (SIE_1_Non_2015_EN.txt)\n",
      "Processing Text (english) 64 of 209 (MUL_22_Non_2017_EN.txt)\n",
      "Processing Text (english) 65 of 209 (EVEN_21_2015_EN.txt)\n",
      "Processing Text (english) 66 of 209 (AEDU_11_2014_EN.txt)\n",
      "Processing Text (english) 67 of 209 (PINE_14_2014_EN.txt)\n",
      "Processing Text (english) 68 of 209 (CPLE_2_2012_EN.txt)\n",
      "Processing Text (english) 69 of 209 (CCRO_1_2015_EN.txt)\n",
      "Processing Text (english) 70 of 209 (FIBR_7_2012_EN.txt)\n",
      "Processing Text (english) 71 of 209 (WAL_25_Non_2013_EN.txt)\n",
      "Processing Text (english) 72 of 209 (EMBR_19_2014_EN.txt)\n",
      "Processing Text (english) 73 of 209 (BRFS_8_2011_EN.txt)\n",
      "Processing Text (english) 74 of 209 (BUN_8_Non_2014_EN.txt)\n",
      "Processing Text (english) 75 of 209 (ITEC_23_2012_EN.txt)\n",
      "Processing Text (english) 76 of 209 (QGEP_10_2015_EN.txt)\n",
      "Processing Text (english) 77 of 209 (JBSS_8_2014_EN.txt)\n",
      "Processing Text (english) 78 of 209 (CBA_3_Non_2018_EN.txt)\n",
      "Processing Text (english) 79 of 209 (PSSA_14_2014_EN.txt)\n",
      "Processing Text (english) 80 of 209 (PETR_10_2011_EN.txt)\n",
      "Processing Text (english) 81 of 209 (EDP_2_Non_2015_EN.txt)\n",
      "Processing Text (english) 82 of 209 (PRE_14_Non_2012_EN.txt)\n",
      "Processing Text (english) 83 of 209 (RSID_21_2011_EN.txt)\n",
      "Processing Text (english) 84 of 209 (INS_11_Non_2017_EN.txt)\n",
      "Processing Text (english) 85 of 209 (BTOW_13_2014_EN.txt)\n",
      "Processing Text (english) 86 of 209 (KPM_14_Non_2016_EN.txt)\n",
      "Processing Text (english) 87 of 209 (ITUB_14_2015_EN.txt)\n",
      "Processing Text (english) 88 of 209 (JBSS_8_2016_EN.txt)\n",
      "Processing Text (english) 89 of 209 (SIC_14_Non_2015_EN.txt)\n",
      "Processing Text (english) 90 of 209 (CESP_2_2013_EN.txt)\n",
      "Processing Text (english) 91 of 209 (CMIG_2_2011_EN.txt)\n",
      "Processing Text (english) 92 of 209 (ELEK_16_2015_EN.txt)\n",
      "Processing Text (english) 93 of 209 (NATU_15_2011_EN.txt)\n",
      "Processing Text (english) 94 of 209 (V_M_3_Non_2011_EN.txt)\n",
      "Processing Text (english) 95 of 209 (BBAS_14_2012_EN.txt)\n",
      "Processing Text (english) 96 of 209 (B3SA_14_2012_EN.txt)\n",
      "Processing Text (english) 97 of 209 (ELET_2_2017_EN.txt)\n",
      "Processing Text (english) 98 of 209 (EMB_6_Non_2012_EN.txt)\n",
      "Processing Text (english) 99 of 209 (‎BRKM_16_2016_EN.txt)\n",
      "Processing Text (english) 100 of 209 (CRUZ_31_2013_EN.txt)\n",
      "Processing Text (english) 101 of 209 (ALLL_5_2014_EN.txt)\n",
      "Processing Text (english) 102 of 209 (GRU_12_Non_2017_EN.txt)\n",
      "Processing Text (english) 103 of 209 (B3SA_14_2017_EN.txt)\n",
      "Processing Text (english) 104 of 209 (MRVE_21_2017_EN.txt)\n",
      "Processing Text (english) 105 of 209 (ITSA_1_2015_EN.txt)\n",
      "Processing Text (english) 106 of 209 (V_M_3_Non_2014_EN.txt)\n",
      "Processing Text (english) 107 of 209 (CPFL_RE_10_2017.txt)\n",
      "Processing Text (english) 108 of 209 (NATU_15_2014_EN.txt)\n",
      "Processing Text (english) 109 of 209 (IHA_16_Non_2014.txt)\n",
      "Processing Text (english) 110 of 209 (WHRL_1_2011_EN.txt)\n",
      "Processing Text (english) 111 of 209 (CESP_2_2016_EN.txt)\n",
      "Processing Text (english) 112 of 209 (DTEX_7_2012_EN.txt)\n",
      "Processing Text (english) 113 of 209 (TIMP_4_2015_EN.txt)\n",
      "Processing Text (english) 114 of 209 (EUR_16_Non_2016_EN.txt)\n",
      "Processing Text (english) 115 of 209 (SUZ_7_Non_2012_EN.txt)\n",
      "Processing Text (english) 116 of 209 (CPLE_2_2015_EN.txt)\n",
      "Processing Text (english) 117 of 209 (AMA_12_Non_2014_EN.txt)\n",
      "Processing Text (english) 118 of 209 (UNI_24_Non_2011_EN.txt)\n",
      "Processing Text (english) 119 of 209 (TIET S_2_2012_EN.txt)\n",
      "Processing Text (english) 120 of 209 (HSB_14_Non_2011_EN.txt)\n",
      "Processing Text (english) 121 of 209 (ECOR_12_2017_EN.txt)\n",
      "Processing Text (english) 122 of 209 (VALE_26_2017_EN.txt)\n",
      "Processing Text (english) 123 of 209 (CCRO_1_2012_EN.txt)\n",
      "Processing Text (english) 124 of 209 (PETR_10_2014_EN.txt)\n",
      "Processing Text (english) 125 of 209 (COP_24_Non_2012_EN.txt)\n",
      "Processing Text (english) 126 of 209 (EMBR_19_2013_EN.txt)\n",
      "Processing Text (english) 127 of 209 (APX_13_Non_2016_EN.txt)\n",
      "Processing Text (english) 128 of 209 (BUN_8_Non_2013_EN.txt)\n",
      "Processing Text (english) 129 of 209 (TIET_2_2013_EN.txt)\n",
      "Processing Text (english) 130 of 209 (B3SA_14_2014-EN.txt)\n",
      "Processing Text (english) 131 of 209 (LREN_17_2014_EN.txt)\n",
      "Processing Text (english) 132 of 209 (SIR_9_Non_2017_EN.txt)\n",
      "Processing Text (english) 133 of 209 (FIBR_7_2017_EN.txt)\n",
      "Processing Text (english) 134 of 209 (CIEL_14_2015_EN.txt)\n",
      "Processing Text (english) 135 of 209 (VALE_26_2015_EN.txt)\n",
      "Processing Text (english) 136 of 209 (SBSP_18_2012_EN.txt)\n",
      "Processing Text (english) 137 of 209 (ECOR_12_2015_EN.txt)\n",
      "Processing Text (english) 138 of 209 (HSB_14_Non_2013_EN.txt)\n",
      "Processing Text (english) 139 of 209 (Syngenta_24_2016_EN.txt)\n",
      "Processing Text (english) 140 of 209 (FDC_11_Non_2017_EN.txt)\n",
      "Processing Text (english) 141 of 209 (ITA_10_Non_2014_EN.txt)\n",
      "Processing Text (english) 142 of 209 (BBDC_14_2012.txt)\n",
      "Processing Text (english) 143 of 209 (NATU_15_2016_EN.txt)\n",
      "Processing Text (english) 144 of 209 (LAME_25_2017_EN.txt)\n",
      "Processing Text (english) 145 of 209 (TIET ELE_2_2015_EN.txt)\n",
      "Processing Text (english) 146 of 209 (KLBN_7_2017_EN.txt)\n",
      "Processing Text (english) 147 of 209 (B3SA_14_2015_EN.txt)\n",
      "Processing Text (english) 148 of 209 (ELE_10_Non_2014_EN.txt)\n",
      "Processing Text (english) 149 of 209 (INV_12_Non_2015_EN.txt)\n",
      "Processing Text (english) 150 of 209 (BBAS_14_2015_EN.txt)\n",
      "Processing Text (english) 151 of 209 (TET_6_Non_2018_EN.txt)\n",
      "Processing Text (english) 152 of 209 (AMA_12_Non_2016_EN.txt)\n",
      "Processing Text (english) 153 of 209 (VIM_1_Non_2015_EN.txt)\n",
      "Processing Text (english) 154 of 209 (SULA_14_2013_EN.txt)\n",
      "Processing Text (english) 155 of 209 (TPIS_5_2013_EN.txt)\n",
      "Processing Text (english) 156 of 209 (EUR_16_Non_2014_EN.txt)\n",
      "Processing Text (english) 157 of 209 (EGIE_10_2015_EN.txt)\n",
      "Processing Text (english) 158 of 209 (ALLL_5_2013_EN.txt)\n",
      "Processing Text (english) 159 of 209 (QGEP_10_2014_EN.txt)\n",
      "Processing Text (english) 160 of 209 (ITEC_23_2013_EN.txt)\n",
      "Processing Text (english) 161 of 209 (CPFE_10_2012_EN.txt)\n",
      "Processing Text (english) 162 of 209 (RNEW_2_2016_EN.txt)\n",
      "Processing Text (english) 163 of 209 (EMBR_19_2015_EN.txt)\n",
      "Processing Text (english) 164 of 209 (LREN_17_2012_EN.txt)\n",
      "Processing Text (english) 165 of 209 (ITUB_14_2016_EN.txt)\n",
      "Processing Text (english) 166 of 209 (COC_8_Non_2012_EN.txt)\n",
      "Processing Text (english) 167 of 209 (WAL_25_Non_2012_EN.txt)\n",
      "Processing Text (english) 168 of 209 (COP_24_Non_2014_EN.txt)\n",
      "Processing Text (english) 169 of 209 (ETER_20_2013_EN.txt)\n",
      "Processing Text (english) 170 of 209 (VALE_26_2011_EN.txt)\n",
      "Processing Text (english) 171 of 209 (MUL_22_Non_2016_EN.txt)\n",
      "Processing Text (english) 172 of 209 (INP_24_Non_2014_EN.txt)\n",
      "Processing Text (english) 173 of 209 (‎BRKM_16_2015_EN.txt)\n",
      "Processing Text (english) 174 of 209 (OXIT_16_2017_EN.txt)\n",
      "Processing Text (english) 175 of 209 (GRU_15_Non_2013_EN.txt)\n",
      "Processing Text (english) 176 of 209 (DTEX_7_2014_EN.txt)\n",
      "Processing Text (english) 177 of 209 (POR_5_Non_2014_EN.txt)\n",
      "Processing Text (english) 178 of 209 (CPLE_2_2013_EN.txt)\n",
      "Processing Text (english) 179 of 209 (BRA_8_Non_2015_EN.txt)\n",
      "Processing Text (english) 180 of 209 (V_M_3_Non_2012_EN.txt)\n",
      "Processing Text (english) 181 of 209 (ITSA_1_2013_EN.txt)\n",
      "Processing Text (english) 182 of 209 (BRSR_14_2013_EN.txt)\n",
      "Processing Text (english) 183 of 209 (FEB_14_Non_2012.txt)\n",
      "Processing Text (english) 184 of 209 (ELET_2_2014_EN.txt)\n",
      "Processing Text (english) 185 of 209 (TAMM_19_2012_EN.txt)\n",
      "Processing Text (english) 186 of 209 (CMIG_2_2012_EN.txt)\n",
      "Processing Text (english) 187 of 209 (NATU_15_2012_EN.txt)\n",
      "Processing Text (english) 188 of 209 (Givaudan_16_2014_EN.txt)\n",
      "Processing Text (english) 189 of 209 (VIM_1_Non_2013_EN.txt)\n",
      "Processing Text (english) 190 of 209 (TIET S_2_2016_EN.txt)\n",
      "Processing Text (english) 191 of 209 (ODE_1_Non_2014.txt)\n",
      "Processing Text (english) 192 of 209 (OXIT_16_2015_EN.txt)\n",
      "Processing Text (english) 193 of 209 (VER_7_Non_2012_EN.txt)\n",
      "Processing Text (english) 194 of 209 (‎BRKM_16_2017_EN.txt)\n",
      "Processing Text (english) 195 of 209 (BBAS_14_2013_EN.txt)\n",
      "Processing Text (english) 196 of 209 (INV_12_Non_2013_EN.txt)\n",
      "Processing Text (english) 197 of 209 (DOW_16_Non_2014_EN.txt)\n",
      "Processing Text (english) 198 of 209 (CSAN_12_2014_EN.txt)\n",
      "Processing Text (english) 199 of 209 (BRFS_8_2012_EN.txt)\n",
      "Processing Text (english) 200 of 209 (MGLU_12_2015_EN.txt)\n",
      "Processing Text (english) 201 of 209 (JBSS_8_2017_EN.txt)\n",
      "Processing Text (english) 202 of 209 (CPFE_10_2017.txt)\n",
      "Processing Text (english) 203 of 209 (INS_11_Non_2016_EN.txt)\n",
      "Processing Text (english) 204 of 209 (VALE_26_2013_EN.txt)\n",
      "Processing Text (english) 205 of 209 (CCRO_1_2016_EN.txt)\n",
      "Processing Text (english) 206 of 209 (FIBR_7_2011_EN.txt)\n",
      "Processing Text (english) 207 of 209 (PRE_14_Non_2013_EN.txt)\n",
      "Processing Text (english) 208 of 209 (PETR_10_2012.txt)\n",
      "Processing Text (english) 209 of 209 (VIS_12_Non_2013_EN.txt)\n",
      "Finishing filter_special_keywords\n",
      "CPU times: user 35.5 s, sys: 1.19 s, total: 36.7 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "                            filter_list_file='lista_palavras_EN.txt', \n",
    "                            raw_text_path='./raw_texts/EN/',\n",
    "                            output_path='./reports_special/EN/',\n",
    "                            output_file='relatorio_normal_frases_EN.csv',\n",
    "                            word_match_count=count_normal,\n",
    "                            confidence=-1,\n",
    "                            phrases=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting filter_special_keywords\n",
      "successfully read the filter words at /Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/ named lista_palavras_EN.txt\n",
      "Processing Text (english) 1 of 209 (CPFL_RE_10_2014_EN.txt)\n",
      "Processing Text (english) 2 of 209 (ESTC_11_2015_EN.txt)\n",
      "Processing Text (english) 3 of 209 (BUN_8_Non_2012_EN.txt)\n",
      "Processing Text (english) 4 of 209 (EMBR_19_2012_EN.txt)\n",
      "Processing Text (english) 5 of 209 (LREN_17_2015_EN.txt)\n",
      "Processing Text (english) 6 of 209 (Syngenta_24_2015_EN.txt)\n",
      "Processing Text (english) 7 of 209 (TCSA_21_2017_EN.txt)\n",
      "Processing Text (english) 8 of 209 (ALI_14_Non_2015_EN.txt)\n",
      "Processing Text (english) 9 of 209 (END_2_Non_2011_EN.txt)\n",
      "Processing Text (english) 10 of 209 (FIBR_7_2014_EN.txt)\n",
      "Processing Text (english) 11 of 209 (SANB_14_2011_EN.txt)\n",
      "Processing Text (english) 12 of 209 (IMA_33_Non_2013_EN.txt)\n",
      "Processing Text (english) 13 of 209 (CIEL_14_2016_EN.txt)\n",
      "Processing Text (english) 14 of 209 (INP_24_Non_2013_EN.txt)\n",
      "Processing Text (english) 15 of 209 (INS_32_Non_2017.txt)\n",
      "Processing Text (english) 16 of 209 (‎BRKM_16_2012_EN.txt)\n",
      "Processing Text (english) 17 of 209 (POR_5_Non_2013_EN.txt)\n",
      "Processing Text (english) 18 of 209 (TIMP_4_2014_EN.txt)\n",
      "Processing Text (english) 19 of 209 (DTEX_7_2013_EN.txt)\n",
      "Processing Text (english) 20 of 209 (AMA_12_Non_2015_EN.txt)\n",
      "Processing Text (english) 21 of 209 (CPLE_2_2014_EN.txt)\n",
      "Processing Text (english) 22 of 209 (USI_10_Non_2012_EN.txt)\n",
      "Processing Text (english) 23 of 209 (KLBN_7_2014_EN.txt)\n",
      "Processing Text (english) 24 of 209 (ITSA_1_2014_EN.txt)\n",
      "Processing Text (english) 25 of 209 (ABEV_8_2016_EN.txt)\n",
      "Processing Text (english) 26 of 209 (VOT_20_Non_2015_EN.txt)\n",
      "Processing Text (english) 27 of 209 (BBAS_14_2016_EN.txt)\n",
      "Processing Text (english) 28 of 209 (GRU_12_Non_2016_EN.txt)\n",
      "Processing Text (english) 29 of 209 (B3SA_14_2016_EN.txt)\n",
      "Processing Text (english) 30 of 209 (CESP_2_2017_EN.txt)\n",
      "Processing Text (english) 31 of 209 (SUZ_7_Non_2011_EN.txt)\n",
      "Processing Text (english) 32 of 209 (Givaudan_16_2013_EN.txt)\n",
      "Processing Text (english) 33 of 209 (CPLE_2_2016_EN.txt)\n",
      "Processing Text (english) 34 of 209 (EGIE_10_2014_EN.txt)\n",
      "Processing Text (english) 35 of 209 (EUR_16_Non_2015_EN.txt)\n",
      "Processing Text (english) 36 of 209 (EMB_6_Non_2014_EN.txt)\n",
      "Processing Text (english) 37 of 209 (LAME_25_2016_EN.txt)\n",
      "Processing Text (english) 38 of 209 (ELEK_16_2013_EN.txt)\n",
      "Processing Text (english) 39 of 209 (CESP_2_2015_EN.txt)\n",
      "Processing Text (english) 40 of 209 (WHRL_1_2012_EN.txt)\n",
      "Processing Text (english) 41 of 209 (BBAS_14_2014_EN.txt)\n",
      "Processing Text (english) 42 of 209 (DOW_16_Non_2013_EN.txt)\n",
      "Processing Text (english) 43 of 209 (ROMI_23_2012_EN.txt)\n",
      "Processing Text (english) 44 of 209 (Syngenta_24_2017_EN.txt)\n",
      "Processing Text (english) 45 of 209 (FDC_11_Non_2014.txt)\n",
      "Processing Text (english) 46 of 209 (BRFS_8_2015_EN.txt)\n",
      "Processing Text (english) 47 of 209 (VALE_26_2014_EN.txt)\n",
      "Processing Text (english) 48 of 209 (CCRO_1_2011_EN.txt)\n",
      "Processing Text (english) 49 of 209 (C_A_25_Non_2012_EN.txt)\n",
      "Processing Text (english) 50 of 209 (PRE_14_Non_2014_EN.txt)\n",
      "Processing Text (english) 51 of 209 (EDP_2_Non_2013_EN.txt)\n",
      "Processing Text (english) 52 of 209 (BEEF_8_2014_EN.txt)\n",
      "Processing Text (english) 53 of 209 (BRSR_14_2012_EN.txt)\n",
      "Processing Text (english) 54 of 209 (USI_10_Non_2014_EN.txt)\n",
      "Processing Text (english) 55 of 209 (V_M_3_Non_2013_EN.txt)\n",
      "Processing Text (english) 56 of 209 (NATU_15_2013_EN.txt)\n",
      "Processing Text (english) 57 of 209 (CMIG_2_2013_EN.txt)\n",
      "Processing Text (english) 58 of 209 (ARMT_3_2015_EN.txt)\n",
      "Processing Text (english) 59 of 209 (POR_5_Non_2015_EN.txt)\n",
      "Processing Text (english) 60 of 209 (DTEX_7_2015_EN.txt)\n",
      "Processing Text (english) 61 of 209 (VER_7_Non_2011_EN.txt)\n",
      "Processing Text (english) 62 of 209 (CAR_24_Non_2016_EN.txt)\n",
      "Processing Text (english) 63 of 209 (SIE_1_Non_2015_EN.txt)\n",
      "Processing Text (english) 64 of 209 (MUL_22_Non_2017_EN.txt)\n",
      "Processing Text (english) 65 of 209 (EVEN_21_2015_EN.txt)\n",
      "Processing Text (english) 66 of 209 (AEDU_11_2014_EN.txt)\n",
      "Processing Text (english) 67 of 209 (PINE_14_2014_EN.txt)\n",
      "Processing Text (english) 68 of 209 (CPLE_2_2012_EN.txt)\n",
      "Processing Text (english) 69 of 209 (CCRO_1_2015_EN.txt)\n",
      "Processing Text (english) 70 of 209 (FIBR_7_2012_EN.txt)\n",
      "Processing Text (english) 71 of 209 (WAL_25_Non_2013_EN.txt)\n",
      "Processing Text (english) 72 of 209 (EMBR_19_2014_EN.txt)\n",
      "Processing Text (english) 73 of 209 (BRFS_8_2011_EN.txt)\n",
      "Processing Text (english) 74 of 209 (BUN_8_Non_2014_EN.txt)\n",
      "Processing Text (english) 75 of 209 (ITEC_23_2012_EN.txt)\n",
      "Processing Text (english) 76 of 209 (QGEP_10_2015_EN.txt)\n",
      "Processing Text (english) 77 of 209 (JBSS_8_2014_EN.txt)\n",
      "Processing Text (english) 78 of 209 (CBA_3_Non_2018_EN.txt)\n",
      "Processing Text (english) 79 of 209 (PSSA_14_2014_EN.txt)\n",
      "Processing Text (english) 80 of 209 (PETR_10_2011_EN.txt)\n",
      "Processing Text (english) 81 of 209 (EDP_2_Non_2015_EN.txt)\n",
      "Processing Text (english) 82 of 209 (PRE_14_Non_2012_EN.txt)\n",
      "Processing Text (english) 83 of 209 (RSID_21_2011_EN.txt)\n",
      "Processing Text (english) 84 of 209 (INS_11_Non_2017_EN.txt)\n",
      "Processing Text (english) 85 of 209 (BTOW_13_2014_EN.txt)\n",
      "Processing Text (english) 86 of 209 (KPM_14_Non_2016_EN.txt)\n",
      "Processing Text (english) 87 of 209 (ITUB_14_2015_EN.txt)\n",
      "Processing Text (english) 88 of 209 (JBSS_8_2016_EN.txt)\n",
      "Processing Text (english) 89 of 209 (SIC_14_Non_2015_EN.txt)\n",
      "Processing Text (english) 90 of 209 (CESP_2_2013_EN.txt)\n",
      "Processing Text (english) 91 of 209 (CMIG_2_2011_EN.txt)\n",
      "Processing Text (english) 92 of 209 (ELEK_16_2015_EN.txt)\n",
      "Processing Text (english) 93 of 209 (NATU_15_2011_EN.txt)\n",
      "Processing Text (english) 94 of 209 (V_M_3_Non_2011_EN.txt)\n",
      "Processing Text (english) 95 of 209 (BBAS_14_2012_EN.txt)\n",
      "Processing Text (english) 96 of 209 (B3SA_14_2012_EN.txt)\n",
      "Processing Text (english) 97 of 209 (ELET_2_2017_EN.txt)\n",
      "Processing Text (english) 98 of 209 (EMB_6_Non_2012_EN.txt)\n",
      "Processing Text (english) 99 of 209 (‎BRKM_16_2016_EN.txt)\n",
      "Processing Text (english) 100 of 209 (CRUZ_31_2013_EN.txt)\n",
      "Processing Text (english) 101 of 209 (ALLL_5_2014_EN.txt)\n",
      "Processing Text (english) 102 of 209 (GRU_12_Non_2017_EN.txt)\n",
      "Processing Text (english) 103 of 209 (B3SA_14_2017_EN.txt)\n",
      "Processing Text (english) 104 of 209 (MRVE_21_2017_EN.txt)\n",
      "Processing Text (english) 105 of 209 (ITSA_1_2015_EN.txt)\n",
      "Processing Text (english) 106 of 209 (V_M_3_Non_2014_EN.txt)\n",
      "Processing Text (english) 107 of 209 (CPFL_RE_10_2017.txt)\n",
      "Processing Text (english) 108 of 209 (NATU_15_2014_EN.txt)\n",
      "Processing Text (english) 109 of 209 (IHA_16_Non_2014.txt)\n",
      "Processing Text (english) 110 of 209 (WHRL_1_2011_EN.txt)\n",
      "Processing Text (english) 111 of 209 (CESP_2_2016_EN.txt)\n",
      "Processing Text (english) 112 of 209 (DTEX_7_2012_EN.txt)\n",
      "Processing Text (english) 113 of 209 (TIMP_4_2015_EN.txt)\n",
      "Processing Text (english) 114 of 209 (EUR_16_Non_2016_EN.txt)\n",
      "Processing Text (english) 115 of 209 (SUZ_7_Non_2012_EN.txt)\n",
      "Processing Text (english) 116 of 209 (CPLE_2_2015_EN.txt)\n",
      "Processing Text (english) 117 of 209 (AMA_12_Non_2014_EN.txt)\n",
      "Processing Text (english) 118 of 209 (UNI_24_Non_2011_EN.txt)\n",
      "Processing Text (english) 119 of 209 (TIET S_2_2012_EN.txt)\n",
      "Processing Text (english) 120 of 209 (HSB_14_Non_2011_EN.txt)\n",
      "Processing Text (english) 121 of 209 (ECOR_12_2017_EN.txt)\n",
      "Processing Text (english) 122 of 209 (VALE_26_2017_EN.txt)\n",
      "Processing Text (english) 123 of 209 (CCRO_1_2012_EN.txt)\n",
      "Processing Text (english) 124 of 209 (PETR_10_2014_EN.txt)\n",
      "Processing Text (english) 125 of 209 (COP_24_Non_2012_EN.txt)\n",
      "Processing Text (english) 126 of 209 (EMBR_19_2013_EN.txt)\n",
      "Processing Text (english) 127 of 209 (APX_13_Non_2016_EN.txt)\n",
      "Processing Text (english) 128 of 209 (BUN_8_Non_2013_EN.txt)\n",
      "Processing Text (english) 129 of 209 (TIET_2_2013_EN.txt)\n",
      "Processing Text (english) 130 of 209 (B3SA_14_2014-EN.txt)\n",
      "Processing Text (english) 131 of 209 (LREN_17_2014_EN.txt)\n",
      "Processing Text (english) 132 of 209 (SIR_9_Non_2017_EN.txt)\n",
      "Processing Text (english) 133 of 209 (FIBR_7_2017_EN.txt)\n",
      "Processing Text (english) 134 of 209 (CIEL_14_2015_EN.txt)\n",
      "Processing Text (english) 135 of 209 (VALE_26_2015_EN.txt)\n",
      "Processing Text (english) 136 of 209 (SBSP_18_2012_EN.txt)\n",
      "Processing Text (english) 137 of 209 (ECOR_12_2015_EN.txt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Text (english) 138 of 209 (HSB_14_Non_2013_EN.txt)\n",
      "Processing Text (english) 139 of 209 (Syngenta_24_2016_EN.txt)\n",
      "Processing Text (english) 140 of 209 (FDC_11_Non_2017_EN.txt)\n",
      "Processing Text (english) 141 of 209 (ITA_10_Non_2014_EN.txt)\n",
      "Processing Text (english) 142 of 209 (BBDC_14_2012.txt)\n",
      "Processing Text (english) 143 of 209 (NATU_15_2016_EN.txt)\n",
      "Processing Text (english) 144 of 209 (LAME_25_2017_EN.txt)\n",
      "Processing Text (english) 145 of 209 (TIET ELE_2_2015_EN.txt)\n",
      "Processing Text (english) 146 of 209 (KLBN_7_2017_EN.txt)\n",
      "Processing Text (english) 147 of 209 (B3SA_14_2015_EN.txt)\n",
      "Processing Text (english) 148 of 209 (ELE_10_Non_2014_EN.txt)\n",
      "Processing Text (english) 149 of 209 (INV_12_Non_2015_EN.txt)\n",
      "Processing Text (english) 150 of 209 (BBAS_14_2015_EN.txt)\n",
      "Processing Text (english) 151 of 209 (TET_6_Non_2018_EN.txt)\n",
      "Processing Text (english) 152 of 209 (AMA_12_Non_2016_EN.txt)\n",
      "Processing Text (english) 153 of 209 (VIM_1_Non_2015_EN.txt)\n",
      "Processing Text (english) 154 of 209 (SULA_14_2013_EN.txt)\n",
      "Processing Text (english) 155 of 209 (TPIS_5_2013_EN.txt)\n",
      "Processing Text (english) 156 of 209 (EUR_16_Non_2014_EN.txt)\n",
      "Processing Text (english) 157 of 209 (EGIE_10_2015_EN.txt)\n",
      "Processing Text (english) 158 of 209 (ALLL_5_2013_EN.txt)\n",
      "Processing Text (english) 159 of 209 (QGEP_10_2014_EN.txt)\n",
      "Processing Text (english) 160 of 209 (ITEC_23_2013_EN.txt)\n",
      "Processing Text (english) 161 of 209 (CPFE_10_2012_EN.txt)\n",
      "Processing Text (english) 162 of 209 (RNEW_2_2016_EN.txt)\n",
      "Processing Text (english) 163 of 209 (EMBR_19_2015_EN.txt)\n",
      "Processing Text (english) 164 of 209 (LREN_17_2012_EN.txt)\n",
      "Processing Text (english) 165 of 209 (ITUB_14_2016_EN.txt)\n",
      "Processing Text (english) 166 of 209 (COC_8_Non_2012_EN.txt)\n",
      "Processing Text (english) 167 of 209 (WAL_25_Non_2012_EN.txt)\n",
      "Processing Text (english) 168 of 209 (COP_24_Non_2014_EN.txt)\n",
      "Processing Text (english) 169 of 209 (ETER_20_2013_EN.txt)\n",
      "Processing Text (english) 170 of 209 (VALE_26_2011_EN.txt)\n",
      "Processing Text (english) 171 of 209 (MUL_22_Non_2016_EN.txt)\n",
      "Processing Text (english) 172 of 209 (INP_24_Non_2014_EN.txt)\n",
      "Processing Text (english) 173 of 209 (‎BRKM_16_2015_EN.txt)\n",
      "Processing Text (english) 174 of 209 (OXIT_16_2017_EN.txt)\n",
      "Processing Text (english) 175 of 209 (GRU_15_Non_2013_EN.txt)\n",
      "Processing Text (english) 176 of 209 (DTEX_7_2014_EN.txt)\n",
      "Processing Text (english) 177 of 209 (POR_5_Non_2014_EN.txt)\n",
      "Processing Text (english) 178 of 209 (CPLE_2_2013_EN.txt)\n",
      "Processing Text (english) 179 of 209 (BRA_8_Non_2015_EN.txt)\n",
      "Processing Text (english) 180 of 209 (V_M_3_Non_2012_EN.txt)\n",
      "Processing Text (english) 181 of 209 (ITSA_1_2013_EN.txt)\n",
      "Processing Text (english) 182 of 209 (BRSR_14_2013_EN.txt)\n",
      "Processing Text (english) 183 of 209 (FEB_14_Non_2012.txt)\n",
      "Processing Text (english) 184 of 209 (ELET_2_2014_EN.txt)\n",
      "Processing Text (english) 185 of 209 (TAMM_19_2012_EN.txt)\n",
      "Processing Text (english) 186 of 209 (CMIG_2_2012_EN.txt)\n",
      "Processing Text (english) 187 of 209 (NATU_15_2012_EN.txt)\n",
      "Processing Text (english) 188 of 209 (Givaudan_16_2014_EN.txt)\n",
      "Processing Text (english) 189 of 209 (VIM_1_Non_2013_EN.txt)\n",
      "Processing Text (english) 190 of 209 (TIET S_2_2016_EN.txt)\n",
      "Processing Text (english) 191 of 209 (ODE_1_Non_2014.txt)\n",
      "Processing Text (english) 192 of 209 (OXIT_16_2015_EN.txt)\n",
      "Processing Text (english) 193 of 209 (VER_7_Non_2012_EN.txt)\n",
      "Processing Text (english) 194 of 209 (‎BRKM_16_2017_EN.txt)\n",
      "Processing Text (english) 195 of 209 (BBAS_14_2013_EN.txt)\n",
      "Processing Text (english) 196 of 209 (INV_12_Non_2013_EN.txt)\n",
      "Processing Text (english) 197 of 209 (DOW_16_Non_2014_EN.txt)\n",
      "Processing Text (english) 198 of 209 (CSAN_12_2014_EN.txt)\n",
      "Processing Text (english) 199 of 209 (BRFS_8_2012_EN.txt)\n",
      "Processing Text (english) 200 of 209 (MGLU_12_2015_EN.txt)\n",
      "Processing Text (english) 201 of 209 (JBSS_8_2017_EN.txt)\n",
      "Processing Text (english) 202 of 209 (CPFE_10_2017.txt)\n",
      "Processing Text (english) 203 of 209 (INS_11_Non_2016_EN.txt)\n",
      "Processing Text (english) 204 of 209 (VALE_26_2013_EN.txt)\n",
      "Processing Text (english) 205 of 209 (CCRO_1_2016_EN.txt)\n",
      "Processing Text (english) 206 of 209 (FIBR_7_2011_EN.txt)\n",
      "Processing Text (english) 207 of 209 (PRE_14_Non_2013_EN.txt)\n",
      "Processing Text (english) 208 of 209 (PETR_10_2012.txt)\n",
      "Processing Text (english) 209 of 209 (VIS_12_Non_2013_EN.txt)\n",
      "Finishing filter_special_keywords\n",
      "CPU times: user 1h 43min 49s, sys: 17.1 s, total: 1h 44min 6s\n",
      "Wall time: 1h 45min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## APROXIMADO 95%\n",
    "confidence = 95\n",
    "filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "                            filter_list_file='lista_palavras_EN.txt', \n",
    "                            raw_text_path='./raw_texts/EN/',\n",
    "                            output_path='./reports_special/EN/',\n",
    "                            output_file='relatorio_aproximado({0})_frases_EN.csv'.format(confidence),\n",
    "                            word_match_count=count_fuzzy,\n",
    "                            confidence=confidence,\n",
    "                            phrases=True,\n",
    "                            debug=True,\n",
    "                            output_debug_path='./debug_fuzzy/EN/'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(len(PDFs))\n",
    "# print(len(PDFs_PT))\n",
    "# print(len(PDFs_EN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## APROXIMADO 85%\n",
    "# confidence = 85\n",
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Final_Dissertacao/', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN_dev/',\n",
    "#                             output_path='./reports_special/EN/',\n",
    "#                             output_file='relatorio_aproximado({0})_frases_EN_dev.csv'.format(confidence),\n",
    "#                             word_match_count=count_fuzzy,\n",
    "#                             confidence=confidence,\n",
    "#                             phrases=True,\n",
    "#                             debug=True,\n",
    "#                             output_debug_path='./debug_fuzzy/EN_dev/'\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = u'administrativa da joao Elekeiroz. GRI 2.3  joao| 2.4As vendas para o mercado interno 88 corresponderam a 88% da Receita Líquida da Elekeiroz em 2011. As exportações para\\nmais de 30 países responderam pelos restantes 12%. Os produtos \\norgânicos foram vendidos nos mercados interno e externo, enquanto os inorgânicos tiveram como destino o mercado interno. MISSÃO GRI 4.8  GRI 2.2 | 2.7 | 2.8 Buscar a melhoria contínua dos produtos e serviços oferecidos aos seus clientes internos e \\nexternos; destacar-se em '\n",
    "print('count = {0}'.format(count_total_words(text)))\n",
    "\n",
    "count_normal(text, 'joão', phrases=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keywords = extract_keywords('/Users/hmg/Downloads/', '3M_RS2014.pdf', '/Users/hmg/Downloads/')\n",
    "# keywords_frequency = collections.Counter(keywords)\n",
    "# \n",
    "# keywords_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keywords = extract_keywords('/Users/hmg/Desktop/data/PDF_relatorios_sustentabilidade/FIBR_7_2012_EN.pdf', \n",
    "#                             stop_words_language='english')\n",
    "# keywords_frequency = collections.Counter(keywords)\n",
    "\n",
    "# word_cloud_from_keywords_frequency(keywords_frequency, file_name='hey.png', plot=False)\n",
    "\n",
    "# keywords_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate_statistics(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testCount = 3 and phraseCount = 2\n"
     ]
    }
   ],
   "source": [
    "text = 'test text with test words in the text test words'\n",
    "\n",
    "testCount = count_normal(text, 'test', phrases = False)\n",
    "phraseCount = count_normal(text, 'test words', phrases = True)\n",
    "print('testCount = {0} and phraseCount = {1}'.format(testCount, phraseCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Projeto_tese/', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN/',\n",
    "#                             output_path='./reports_special/EN/')\n",
    "\n",
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Projeto_tese/', \n",
    "filter_special_keywords(filter_list_path='./', \n",
    "                            filter_list_file='lista_frases_v3_dev.txt', \n",
    "                            raw_text_path='./raw_texts/PT_dev/',\n",
    "                            output_path='./reports_special/PT/',\n",
    "                            output_file='output_PT_dev_fuzzy_phrases95.csv',\n",
    "                            word_match_count=count_fuzzy,\n",
    "                            confidence=95,\n",
    "                            phrases=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests with fuzzy string match\n",
    "# fuzz.ratio(str.lower(u'Casarão'), str.lower(u'Caserão'))\n",
    "# fuzz.token_set_ratio(str.lower(u'Casarão'), str.lower(u'Casârão'))\n",
    "\n",
    "# normal_count = count_normal(\"mama mia this is a mama and there is no mia in this mama\", \"mãma\")\n",
    "# fuzzy_count = count_fuzzy(\"mama mia this is a mama and there is no mia in this mama\", \"mãma\", 80)\n",
    "\n",
    "# print('normal = {0}, fuzzy = {1}'.format(normal_count, fuzzy_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(str.lower(u'vacao da biodiversidade'), str.lower(u''))\n",
    "# fuzz.token_set_ratio(str.lower(u'EN-13'), str.lower(u'EN-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
