{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading PDF files and basic NLP\n",
    "import PyPDF2\n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Fuzzy string match\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# To generate word clouds\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Read unicode data from the extra_stop_words file\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Frequency counting and collections\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# To navigate through all files in a directory\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define functions for the text extraction, preprocessing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://gist.github.com/boniattirodrigo/67429ada53b7337d2e79\n",
    "def remove_special_characters(word):\n",
    "\n",
    "    # Unicode normalize transforma um caracter em seu equivalente em latin.\n",
    "    nfkd = unicodedata.normalize('NFKD', word)\n",
    "    plain_word = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "\n",
    "    # return the word with only numbers, letters and spaces\n",
    "    return re.sub('[^a-zA-Z0-9 \\\\\\]', '', plain_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f\n",
    "def extract_keywords(path_pdf,\n",
    "                     pdf_file,\n",
    "                     path_output_raw_text,\n",
    "                     stop_words_language='portuguese', \n",
    "                     path_extra_stop_words='/Users/hmg/Dropbox/veve e heitor/Projeto_tese',\n",
    "                     file_extra_stop_words='extra_stop_words.txt'):\n",
    "    \n",
    "    # Check if the raw text is not already availabe in the path_output_raw_text\n",
    "    text = ''\n",
    "    try:\n",
    "        candidate_raw = open(os.path.join(path_output_raw_text, pdf_file.replace('.pdf', '.txt')), 'r')\n",
    "        text = candidate_raw.read()\n",
    "#         print('successfully read the raw text! ')\n",
    "    except IOError:\n",
    "#         print('gotta scan the pdf...')\n",
    "        # open allows you to read the file\n",
    "        pdfFileObj = open(os.path.join(path_pdf, pdf_file), 'rb')\n",
    "        # The pdfReader variable is a readable object that will be parsed\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        # discerning the number of pages will allow us to parse through all the pages\n",
    "        num_pages = pdfReader.numPages\n",
    "        # Just to show info about creator and creation time. \n",
    "        # print(str(pdfReader.getDocumentInfo())\n",
    "        count = 0\n",
    "\n",
    "        # The while loop will read each page\n",
    "        while count < num_pages:\n",
    "            pageObj = pdfReader.getPage(count)\n",
    "            count += 1\n",
    "            text += pageObj.extractText()\n",
    "        # This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned\n",
    "        # files.\n",
    "        if text != \"\":\n",
    "            text = text\n",
    "        # If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "        else: # fileurl\n",
    "            text = textract.process(os.path.join(path_pdf, pdf_file), method='tesseract', language='eng', encoding='utf8')\n",
    "\n",
    "        file_raw_text = open(os.path.join(path_output_raw_text, pdf_file.replace('.pdf', '.txt')), 'w', encoding='utf8')\n",
    "        file_raw_text.write(text)\n",
    "    \n",
    "#     text = remove_special_characters(text)\n",
    "    # The word_tokenize() function will break our text phrases into #individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '%', '-', '.', '|', '']\n",
    "    stop_words = stopwords.words(stop_words_language)\n",
    "    extra_stop_words = open(os.path.join(path_extra_stop_words, file_extra_stop_words), 'r', encoding='utf8').read().split('\\n')\n",
    "    \n",
    "    # We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN\n",
    "    # punctuations.\n",
    "    keywords = [word.lower() for word in tokens \n",
    "                if \n",
    "                not word.lower() in stop_words and \n",
    "                not word.lower() in punctuations and \n",
    "                not word.lower() in extra_stop_words and \n",
    "                word.isalpha() and\n",
    "                len(word) >= 2]\n",
    "    \n",
    "    return np.asarray(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cloud_from_keywords_frequency(keywords_frequency, file_name, path='./wordclouds/', show_image=False):\n",
    "    wordcloud = WordCloud(width = 512, height = 512, background_color='white')\n",
    "    fig = plt.figure(figsize=(20,16),facecolor = 'white', edgecolor='blue')\n",
    "    plt.imshow(wordcloud.generate_from_frequencies(keywords_frequency), interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.savefig(path+file_name)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def column_plot(keywords_frequency)\n",
    "#     df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
    "#     df.plot.bar(x='Word',y='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_statistics(keywords):\n",
    "    total_words = 0\n",
    "    keywords_dict = dict(collections.Counter(keywords).most_common())\n",
    "    for k in keywords_dict:\n",
    "        total_words += keywords_dict[k]\n",
    "\n",
    "    stats = dict()\n",
    "    for k in keywords_dict:\n",
    "        stats[k] = {'count': keywords_dict[k], 'text_frequency': keywords_dict[k]/float(total_words)}    \n",
    "#     stats['frequency'].most_common(10)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Count occurrences of predefined phrases/words\n",
    "\n",
    "**IN-DEVELOPMENT:** Count occurrences of predefined text using 'fuzzy string match', added ```word_match_count``` lambda object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_normal(text, word, confidence = 0, phrases = False):\n",
    "    return text.count(word)\n",
    "\n",
    "def count_fuzzy(text, word, confidence = 95, phrases=False):\n",
    "    counter = 0\n",
    "    if len(word) == 0:\n",
    "        return 0\n",
    "    # Phrases\n",
    "    if phrases:\n",
    "        # Sliding window strategy: create a text_word from text with length word.len \n",
    "        #  and by moving 1 character at a time\n",
    "        #     If match, then skip the next word.len, just to avoid double counting!\n",
    "        i = 0\n",
    "        ## DEBUG\n",
    "#         print('count_fuzzy - word = {0} and, len(text) = {1}, confidence = {2}'.format(word, len(text), confidence))\n",
    "        print('PALAVRA = {0} - CONFIANCA = {1}'.format(word, confidence))\n",
    "    \n",
    "        while i < len(text):\n",
    "            text_word_last_idx = i+len(word)\n",
    "            match_confidence = 0\n",
    "            if text_word_last_idx <= len(text):\n",
    "                match_confidence = fuzz.ratio(text[i:text_word_last_idx], word)\n",
    "#                 print('token_set_ratio({0},{1})={2}'.format(text[i:text_word_last_idx], word, match_confidence))\n",
    "                if match_confidence > confidence:\n",
    "                    counter += 1\n",
    "                    print('{0} =~ {1} ({2})'.format(text[i:text_word_last_idx], word, match_confidence))\n",
    "                    i += len(word)\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                break\n",
    "            ## DEBUG - only about 100 characters\n",
    "            if(i % int(len(text)/10) == 0):\n",
    "                print('{} of {} = {:.2f}%'.format(i, len(text), i/len(text)*100))\n",
    "    else:\n",
    "        text_vec = text.split()\n",
    "        for w in text_vec:\n",
    "            match_confidence = fuzz.ratio(w, word)\n",
    "            if match_confidence > confidence:\n",
    "                counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Projeto_tese/', \n",
    "                            filter_list_file='lista_palavras_EN.txt', \n",
    "                            raw_text_path='./raw_texts/EN/',\n",
    "                            output_path='./reports_special/EN/',\n",
    "                            output_file='output_EN.csv',\n",
    "                            word_match_count=count_normal,\n",
    "                            confidence=95,\n",
    "                            phrases=False):\n",
    "    print('Starting filter_special_keywords')\n",
    "    filter_words = ''\n",
    "    try:\n",
    "        filter_words = open(os.path.join(filter_list_path, filter_list_file), 'r', encoding='utf8').read().split('\\n')\n",
    "        print('successfully read the filter words at {0} named {1}'.format(filter_list_path, filter_list_file))\n",
    "    except IOError:\n",
    "        print('failed to read the filter words at {0} named {1}'.format(filter_list_path, filter_list_file))\n",
    "    \n",
    "    # assuming there are only the raw texts in the directory\n",
    "    text_files = [f for f in listdir(raw_text_path) if isfile(join(raw_text_path, f))]\n",
    "    text_files = list(filter(lambda text_file: text_file.find('.txt') != -1, text_files))\n",
    "    \n",
    "    stats_per_file = {}\n",
    "    for text_file in text_files:\n",
    "        try:\n",
    "            text = open(os.path.join(raw_text_path, text_file), 'r').read()\n",
    "        except IOError:\n",
    "            print('Failed to open file at {0} named {1}'.format(raw_text_path, text_file))\n",
    "#         print('Text file: {0}'.format(text_file))\n",
    "        \n",
    "        stats = {}\n",
    "        for filter_word in filter_words:\n",
    "            stats[filter_word] = word_match_count(text.lower(), filter_word.lower(), confidence, phrases)\n",
    "            # text.lower().count(filter_word.lower())\n",
    "#             line = u'%s,%d,%.5f\\n' % (filter_word, counter, -1)\n",
    "\n",
    "        stats_per_file[text_file.replace('.txt','')] = stats\n",
    "        \n",
    "    output = open(output_path+output_file, 'w', encoding='utf8')   \n",
    "    header = u'word,' + u','.join(text_files)+u'\\n'\n",
    "    output.write(header)\n",
    "    \n",
    "    \n",
    "    for filter_word in filter_words:\n",
    "        output.write(filter_word + ',')\n",
    "        for text_file in text_files:\n",
    "#             line = u'%s,%d,%.5f\\n' % (filter_word, counter, -1)\n",
    "            output.write(str(stats_per_file[text_file.replace('.txt','')][filter_word]) + ',')\n",
    "        output.write('\\n')\n",
    "    output.close()\n",
    "    print('Finishing filter_special_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_report(path, output_file, keyword_stats):\n",
    "    output = open(path+output_file, 'w', encoding='utf8')\n",
    "    header = u'word,count,text_frequency\\n'\n",
    "    output.write(header)\n",
    "    for k in keyword_stats:\n",
    "        line = u'%s,%d,%.5f\\n' % (k, keyword_stats[k]['count'], keyword_stats[k]['text_frequency'])\n",
    "        output.write(line)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process all PDFs in a given path\n",
    "\n",
    "**TODO:** This version of the code still doesn't handle pdfs that are in PT and EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_all_PDFs(path='/Users/hmg/Desktop/data/PDF_relatorios_sustentabilidade/', \n",
    "                     raw_text_path='./raw_texts/',\n",
    "                     output_path='./reports/', \n",
    "                     wordclouds_path='./wordclouds/',\n",
    "                     wordclouds=False):\n",
    "    # We assume there are only pdfs in this directory\n",
    "    PDFs = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    PDFs = list(filter(lambda pdf: pdf.find('.pdf') != -1, PDFs))\n",
    "    \n",
    "    PDFs_PT = list(filter(lambda pdf: pdf.find('_EN.pdf') == -1, PDFs))\n",
    "    PDFs_EN = list(filter(lambda pdf: pdf.find('_EN.pdf') != -1, PDFs))\n",
    "    \n",
    "#     counter = 0 # until 259 they process\n",
    "#     for pdf in PDFs_PT[counter:]:\n",
    "#         counter = counter + 1\n",
    "#         print('Processing PDFs_PT {0} of {1} ({2})'.format(counter, len(PDFs_PT), pdf))\n",
    "#         keywords = extract_keywords(path, pdf, path_output_raw_text=raw_text_path+'PT/')\n",
    "#         keywords_statistics = calculate_statistics(keywords)\n",
    "#         write_report(output_path+'PT/', pdf.replace('.pdf', '.csv'), keywords_statistics)\n",
    "        \n",
    "#         if wordclouds:\n",
    "#             word_cloud_from_keywords_frequency(collections.Counter(keywords), \n",
    "#                                                pdf.replace('.pdf', '.png'), \n",
    "#                                                path=wordclouds_path+'PT/',\n",
    "#                                                show_image=False)\n",
    "        \n",
    "    counter = 0 # until 84 they process\n",
    "    for pdf in PDFs_EN[counter:]:\n",
    "        \n",
    "        print('Processing PDFs_EN {0} of {1} ({2})'.format(counter, len(PDFs_EN), pdf))\n",
    "        keywords = extract_keywords(path, pdf, path_output_raw_text=raw_text_path+'EN/', stop_words_language='english')\n",
    "        keywords_statistics = calculate_statistics(keywords)\n",
    "        write_report(output_path+'EN/', pdf.replace('.pdf', '.csv'), keywords_statistics)\n",
    "        counter = counter + 1\n",
    "        if wordclouds:\n",
    "            word_cloud_from_keywords_frequency(collections.Counter(keywords), \n",
    "                                               pdf.replace('.pdf', '.png'), \n",
    "                                               path=wordclouds_path+'EN/',\n",
    "                                               show_image=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDFs_PT 1 of 1 (B3SA_14_2014-EN.pdf)\n",
      "Processing PDFs_EN 0 of 235 (ERN_13_Non_2012_EN.pdf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ff55f9d1ea76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_all_PDFs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Users/hmg/Desktop/Data/pdfs_EN/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordclouds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-5afc88e5e313>\u001b[0m in \u001b[0;36mprocess_all_PDFs\u001b[0;34m(path, raw_text_path, output_path, wordclouds_path, wordclouds)\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                                \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordclouds_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'EN/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                                show_image=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-463d463ddadb>\u001b[0m in \u001b[0;36mword_cloud_from_keywords_frequency\u001b[0;34m(keywords_frequency, file_name, path, show_image)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfacecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pdf_extraction/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0;32m--> 383\u001b[0;31m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1894ecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_all_PDFs(path='/Users/hmg/Desktop/Data/pdfs_EN/',wordclouds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(len(PDFs))\n",
    "# print(len(PDFs_PT))\n",
    "# print(len(PDFs_EN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keywords = extract_keywords('/Users/hmg/Downloads/', '3M_RS2014.pdf', '/Users/hmg/Downloads/')\n",
    "# keywords_frequency = collections.Counter(keywords)\n",
    "# \n",
    "# keywords_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keywords = extract_keywords('/Users/hmg/Desktop/data/PDF_relatorios_sustentabilidade/FIBR_7_2012_EN.pdf', \n",
    "#                             stop_words_language='english')\n",
    "# keywords_frequency = collections.Counter(keywords)\n",
    "\n",
    "# word_cloud_from_keywords_frequency(keywords_frequency, file_name='hey.png', plot=False)\n",
    "\n",
    "# keywords_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate_statistics(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting filter_special_keywords\n",
      "successfully read the filter words at ./ named lista_frases_v3_dev.txt\n",
      "PALAVRA = impactos na biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = valor da biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = gestão da biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = estratégia de biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = análise da biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = perda da biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = preservação da biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = conservação da biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      " conservação da biodiversidad =~ conservação da biodiversidade (97)\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = proteção da biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = corredor de biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = defesa da biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = área de proteção - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = áreas de alto índice de biodiversidade - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = área de preservação - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = áreas verdes - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = área preservada - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = área afetada - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = área degradada - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = área desmatada - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = área impactada - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = área restaurada - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = habitats naturais - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = habitat restaurado - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = habitat protegido - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = habitat de substituição - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = habitat afetados - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = habitats protegidos e restaurados - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = impactos em habitats - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = habitats protegidos ou restaurados - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = diversidade biológica - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = diversidade genética - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = animais selvagens - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = árvores nativas - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "árvores nativas =~ árvores nativas (100)\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = mata nativa - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = inventário natural - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "PALAVRA = inventário de espécies - CONFIANCA = 95\n",
      "7174 of 71747 = 10.00%\n",
      "14348 of 71747 = 20.00%\n",
      "21522 of 71747 = 30.00%\n",
      "28696 of 71747 = 40.00%\n",
      "35870 of 71747 = 50.00%\n",
      "43044 of 71747 = 59.99%\n",
      "50218 of 71747 = 69.99%\n",
      "57392 of 71747 = 79.99%\n",
      "64566 of 71747 = 89.99%\n",
      "Finishing filter_special_keywords\n"
     ]
    }
   ],
   "source": [
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Projeto_tese/', \n",
    "#                             filter_list_file='lista_palavras_EN.txt', \n",
    "#                             raw_text_path='./raw_texts/EN/',\n",
    "#                             output_path='./reports_special/EN/')\n",
    "\n",
    "# filter_special_keywords(filter_list_path='/Users/hmg/Dropbox/veve e heitor/Projeto_tese/', \n",
    "filter_special_keywords(filter_list_path='./', \n",
    "                            filter_list_file='lista_frases_v3_dev.txt', \n",
    "                            raw_text_path='./raw_texts/PT_dev/',\n",
    "                            output_path='./reports_special/PT/',\n",
    "                            output_file='output_PT_dev_fuzzy_phrases95.csv',\n",
    "                            word_match_count=count_fuzzy,\n",
    "                            confidence=95,\n",
    "                            phrases=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests with fuzzy string match\n",
    "# fuzz.ratio(str.lower(u'Casarão'), str.lower(u'Caserão'))\n",
    "# fuzz.token_set_ratio(str.lower(u'Casarão'), str.lower(u'Casârão'))\n",
    "\n",
    "# normal_count = count_normal(\"mama mia this is a mama and there is no mia in this mama\", \"mãma\")\n",
    "# fuzzy_count = count_fuzzy(\"mama mia this is a mama and there is no mia in this mama\", \"mãma\", 80)\n",
    "\n",
    "# print('normal = {0}, fuzzy = {1}'.format(normal_count, fuzzy_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(str.lower(u'vacao da biodiversidade'), str.lower(u''))\n",
    "# fuzz.token_set_ratio(str.lower(u'EN-13'), str.lower(u'EN-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
